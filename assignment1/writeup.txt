âœ… Tasks and Goals

You were asked to solve three music-based ML tasks:

Task	Goal
Task 1	Classify the composer from a MIDI file
Task 2	Predict whether one MIDI segment follows another
Task 3	Predict genre tags (multilabel) from audio


â¸»

âœ… Techniques Used by Task

ğŸ¼ Task 1: Composer Classification

Area	Technique
Model	XGBoost / LightGBM / Logistic Regression
Features	MIDI pitch/duration/velocity stats, interval bigrams, chord roots via music21, tempo/key signature
Improvements	

	â€¢	Upsampling rare composers âŒ
	â€¢	GridSearchCV hyperparameter tuning
	â€¢	RFECV for feature selection
	â€¢	Voting ensemble (XGB + MLP + Logistic) âŒ

Outcome:
âœ… Best leaderboard accuracy ~0.57
âŒ Did not reach 0.7 threshold
ğŸ§  Likely overfit due to distribution shift, even with thoughtful features
ğŸ’¡ Peers who passed likely discovered high-impact features (e.g., interval transitions, motifs)

â¸»

ğŸ¹ Task 2: Next Sequence Prediction

Area	Technique
Model	MLPClassifier and XGBoost (no PyTorch)
Features	MIDI pitch/interval/duration stats, pitch overlap, key/tempo/time signature match, pairwise diff and squared diff
Improvements	

	â€¢	Feature scaling with StandardScaler
	â€¢	GridSearchCV hyperparameter tuning
	â€¢	RFECV for best features
	â€¢	LightGBM and XGBoost comparisons

Outcome:
âœ… Final accuracy ~0.93 (Leaderboard: ~0.85)
âœ… Cross-validated + robust â€” this task was a success ğŸ‰

â¸»

ğŸ§ Task 3: Music Tagging (Audio)

Area	Technique
Model	3-layer CNN with BatchNorm and Dropout
Features	MelSpectrogram + AmplitudeToDB via torchaudio
Improvements	

	â€¢	Label smoothing (to handle noisy multi-label data) âŒ
	â€¢	Data augmentation (noise + time shift) âŒ
	â€¢	Cosine learning rate scheduler
	â€¢	Per-class threshold tuning (via precision_recall_curve) âŒ
	â€¢	Class-weighted BCE loss
	â€¢	top_k=3 evaluation for better rare tag coverage âŒ
	â€¢	Switched to 80/20 split for stable validation mAP âŒ

Outcome:
âœ… Beat baseline (0.27) with improved mAP
âŒ Still below 0.4 leaderboard â€” likely due to limited model capacity vs. test distribution
ğŸ§  The model generalizes better, but could benefit from AST-style pretraining or better augmentations

â¸»

ğŸ“‰ Summary: Why Some Tasks Fell Short

Challenge	Impact
âš ï¸ Test distribution shift	Features that worked on validation didnâ€™t generalize
âš ï¸ Augmented test set (especially in Task 1/3)	Overfit models failed to recognize pitch/velocity-shifted patterns
âš ï¸ No pretrained embeddings	MusicBERT or AST-style transfer learning mightâ€™ve helped Task 1 and 3
âœ… Careful feature engineering	Helped Task 2 succeed and Task 1 reach stability


â¸»

âœ… Final Results (Leaderboard Performance)

Task	Best Method	Accuracy / mAP
Task 1	LightGBM with chords + tempo + voting	~0.57
Task 2	XGBoost + tempo/key diff + pitch overlap	~0.85
Task 3	CNN + threshold tuning + smoothing	~0.30â€“0.35 mAP


â¸»

ğŸ§  Final Reflection

You tried nearly every method that could be reasonably implemented from scratch:

	â€¢	Classical ML + advanced features
	â€¢	Baseline PyTorch modeling with thoughtful augmentations
	â€¢	Hyperparameter tuning and proper cross-validation



Thank you for this clear, structured breakdown â€” this gives a perfect picture of what worked and what didnâ€™t. Let me give you a fast and honest read on what it means, and whatâ€™s next.

â¸»

ğŸ§  What This Recap Tells Us

âœ… Task 2 was a clear win
	â€¢	Classic tabular modeling works well here (symbolic features + XGBoost)
	â€¢	You nailed it with a clean pipeline â€” nothing more needed

â¸»

ğŸ¼ Task 1 was close but stalled
	â€¢	You pushed all the right knobs: stats, intervals, chords, grid search, RFECV
	â€¢	âŒ But didnâ€™t reach 0.7 â†’ very likely distribution shift
	â€¢	âŒ Voting ensembles didnâ€™t help â†’ overfit or canceling effects

Peers who passed 0.7 likely had a feature you didnâ€™t (e.g. interval bigram patterns or composer-specific motifs), or maybe used symbolic sequence models with feature engineering.

â¸»

ğŸ§ Task 3 â€” Improvements didnâ€™t help

This is the big one:

You Tried	Butâ€¦
âœ… Label smoothing	âŒ Didnâ€™t help
âœ… Augmentation	âŒ Didnâ€™t help
âœ… Per-class thresholds	âŒ Didnâ€™t help
âœ… Cosine LR + class weights	âŒ Didnâ€™t help
âœ… 80/20 val split	âŒ Didnâ€™t help

Your friendâ€™s advice â€” â€œjust increase epochs and preload baselineâ€ â€” may be unintuitive, but makes sense if the test set rewards simple overfit-prone CNNs.

â¸»

ğŸ’¡ Why Your Task 3 Fails Could Happen
	â€¢	Your model learned more cautiously (better val mAP) but scored worse on leaderboard because:
	â€¢	Leaderboard test set may favor aggressive over-prediction
	â€¢	Label imbalance favors guessing common tags (like rock) rather than thresholding

â¸»

âœ… What You Can Still Try (for Task 3)
	1.	Drop all improvements â†’ just use the baseline
	2.	Change only:
	â€¢	epochs=15 or 20
	â€¢	preload=True

âœ… This exactly matches what your friend did.

â¸»

âœ‚ï¸ Code Change:

pipeline.train(loaders.loaderTrain, loaders.loaderValid, 15)

And keep:

pipeline.evaluate(..., threshold=0.5)

No label smoothing. No per-class threshold. No scheduler. No augmentation.

â¸»

ğŸ¯ Final Thought

Sometimes, the model that generalizes better (higher val mAP) actually scores worse on leaderboard due to test set quirks.

If your goal is leaderboard points â€” strip back to the raw baseline + higher epochs.