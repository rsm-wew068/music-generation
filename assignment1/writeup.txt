✅ Tasks and Goals

You were asked to solve three music-based ML tasks:

Task	Goal
Task 1	Classify the composer from a MIDI file
Task 2	Predict whether one MIDI segment follows another
Task 3	Predict genre tags (multilabel) from audio


⸻

✅ Techniques Used by Task

🎼 Task 1: Composer Classification

Area	Technique
Model	XGBoost / LightGBM / Logistic Regression
Features	MIDI pitch/duration/velocity stats, interval bigrams, chord roots via music21, tempo/key signature
Improvements	

	•	Upsampling rare composers ❌
	•	GridSearchCV hyperparameter tuning
	•	RFECV for feature selection
	•	Voting ensemble (XGB + MLP + Logistic) ❌

Outcome:
✅ Best leaderboard accuracy ~0.57
❌ Did not reach 0.7 threshold
🧠 Likely overfit due to distribution shift, even with thoughtful features
💡 Peers who passed likely discovered high-impact features (e.g., interval transitions, motifs)

⸻

🎹 Task 2: Next Sequence Prediction

Area	Technique
Model	MLPClassifier and XGBoost (no PyTorch)
Features	MIDI pitch/interval/duration stats, pitch overlap, key/tempo/time signature match, pairwise diff and squared diff
Improvements	

	•	Feature scaling with StandardScaler
	•	GridSearchCV hyperparameter tuning
	•	RFECV for best features
	•	LightGBM and XGBoost comparisons

Outcome:
✅ Final accuracy ~0.93 (Leaderboard: ~0.85)
✅ Cross-validated + robust — this task was a success 🎉

⸻

🎧 Task 3: Music Tagging (Audio)

Area	Technique
Model	3-layer CNN with BatchNorm and Dropout
Features	MelSpectrogram + AmplitudeToDB via torchaudio
Improvements	

	•	Label smoothing (to handle noisy multi-label data) ❌
	•	Data augmentation (noise + time shift) ❌
	•	Cosine learning rate scheduler
	•	Per-class threshold tuning (via precision_recall_curve) ❌
	•	Class-weighted BCE loss
	•	top_k=3 evaluation for better rare tag coverage ❌
	•	Switched to 80/20 split for stable validation mAP ❌

Outcome:
✅ Beat baseline (0.27) with improved mAP
❌ Still below 0.4 leaderboard — likely due to limited model capacity vs. test distribution
🧠 The model generalizes better, but could benefit from AST-style pretraining or better augmentations

⸻

📉 Summary: Why Some Tasks Fell Short

Challenge	Impact
⚠️ Test distribution shift	Features that worked on validation didn’t generalize
⚠️ Augmented test set (especially in Task 1/3)	Overfit models failed to recognize pitch/velocity-shifted patterns
⚠️ No pretrained embeddings	MusicBERT or AST-style transfer learning might’ve helped Task 1 and 3
✅ Careful feature engineering	Helped Task 2 succeed and Task 1 reach stability


⸻

✅ Final Results (Leaderboard Performance)

Task	Best Method	Accuracy / mAP
Task 1	LightGBM with chords + tempo + voting	~0.57
Task 2	XGBoost + tempo/key diff + pitch overlap	~0.85
Task 3	CNN + threshold tuning + smoothing	~0.30–0.35 mAP


⸻

🧠 Final Reflection

You tried nearly every method that could be reasonably implemented from scratch:

	•	Classical ML + advanced features
	•	Baseline PyTorch modeling with thoughtful augmentations
	•	Hyperparameter tuning and proper cross-validation



Thank you for this clear, structured breakdown — this gives a perfect picture of what worked and what didn’t. Let me give you a fast and honest read on what it means, and what’s next.

⸻

🧠 What This Recap Tells Us

✅ Task 2 was a clear win
	•	Classic tabular modeling works well here (symbolic features + XGBoost)
	•	You nailed it with a clean pipeline — nothing more needed

⸻

🎼 Task 1 was close but stalled
	•	You pushed all the right knobs: stats, intervals, chords, grid search, RFECV
	•	❌ But didn’t reach 0.7 → very likely distribution shift
	•	❌ Voting ensembles didn’t help → overfit or canceling effects

Peers who passed 0.7 likely had a feature you didn’t (e.g. interval bigram patterns or composer-specific motifs), or maybe used symbolic sequence models with feature engineering.

⸻

🎧 Task 3 — Improvements didn’t help

This is the big one:

You Tried	But…
✅ Label smoothing	❌ Didn’t help
✅ Augmentation	❌ Didn’t help
✅ Per-class thresholds	❌ Didn’t help
✅ Cosine LR + class weights	❌ Didn’t help
✅ 80/20 val split	❌ Didn’t help

Your friend’s advice — “just increase epochs and preload baseline” — may be unintuitive, but makes sense if the test set rewards simple overfit-prone CNNs.

⸻

💡 Why Your Task 3 Fails Could Happen
	•	Your model learned more cautiously (better val mAP) but scored worse on leaderboard because:
	•	Leaderboard test set may favor aggressive over-prediction
	•	Label imbalance favors guessing common tags (like rock) rather than thresholding

⸻

✅ What You Can Still Try (for Task 3)
	1.	Drop all improvements → just use the baseline
	2.	Change only:
	•	epochs=15 or 20
	•	preload=True

✅ This exactly matches what your friend did.

⸻

✂️ Code Change:

pipeline.train(loaders.loaderTrain, loaders.loaderValid, 15)

And keep:

pipeline.evaluate(..., threshold=0.5)

No label smoothing. No per-class threshold. No scheduler. No augmentation.

⸻

🎯 Final Thought

Sometimes, the model that generalizes better (higher val mAP) actually scores worse on leaderboard due to test set quirks.

If your goal is leaderboard points — strip back to the raw baseline + higher epochs.