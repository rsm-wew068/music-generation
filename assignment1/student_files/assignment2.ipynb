{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofdnxvLmtXPv"
      },
      "source": [
        "In this project, our goal is to predict the next note given the previous notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wYx2jacclcJt"
      },
      "outputs": [],
      "source": [
        "# # Install PyTorch (if needed)\n",
        "# !pip install torch torchvision torchaudio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "olDrvB9Rqiw6"
      },
      "outputs": [],
      "source": [
        "# !pip install music21 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoLFDV1qrez8",
        "outputId": "064a9e6d-abf3-447c-f1f0-9f3fe5eab2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nottingham-dataset'...\n",
            "remote: Enumerating objects: 3119, done.\u001b[K\n",
            "remote: Total 3119 (delta 0), reused 0 (delta 0), pack-reused 3119 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3119/3119), 879.17 KiB | 2.27 MiB/s, done.\n",
            "Resolving deltas: 100% (1432/1432), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jukedeck/nottingham-dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ohp3ltszvUvg"
      },
      "outputs": [],
      "source": [
        "# !pip install torch scikit-learn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_JaRhtFt4Yl",
        "outputId": "41a34447-8736-467b-b377-d1d126ce0a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/music21/stream/base.py:3689: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
            "  return self.iter().getElementsByClass(classFilterList)\n"
          ]
        }
      ],
      "source": [
        "# STEP 3: Parse melody for next-note prediction\n",
        "import os\n",
        "from music21 import converter, note\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Parse melodies and build note sequences\n",
        "def parse_melody_sequences(folder_path, max_files=50):\n",
        "    note_seqs = []\n",
        "    file_list = sorted([f for f in os.listdir(folder_path) if f.endswith('.abc')])[:max_files]\n",
        "    for file in file_list:\n",
        "        try:\n",
        "            score = converter.parse(os.path.join(folder_path, file))\n",
        "            notes = [el.name for el in score.flat.notes if isinstance(el, note.Note)]\n",
        "            if len(notes) > 10:\n",
        "                note_seqs.append(notes)\n",
        "        except:\n",
        "            continue\n",
        "    return note_seqs\n",
        "\n",
        "dataset_path = \"/content/nottingham-dataset/ABC_cleaned\"\n",
        "melody_sequences = parse_melody_sequences(dataset_path)\n",
        "\n",
        "# Encode notes\n",
        "all_notes = sorted(set(nt for seq in melody_sequences for nt in seq))\n",
        "note_encoder = LabelEncoder().fit(all_notes)\n",
        "encoded_sequences = [note_encoder.transform(seq) for seq in melody_sequences]\n",
        "\n",
        "# Create (input, target) pairs for next-note prediction\n",
        "seq_length = 8\n",
        "input_seqs, target_seqs = [], []\n",
        "for seq in encoded_sequences:\n",
        "    for i in range(len(seq) - seq_length):\n",
        "        input_seqs.append(seq[i:i+seq_length])\n",
        "        target_seqs.append(seq[i+seq_length])\n",
        "\n",
        "class MelodyContinuationDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
        "\n",
        "dataset = MelodyContinuationDataset(input_seqs, target_seqs)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj2C3es9uN_G"
      },
      "source": [
        "Because this model is already training from scratch, on a relatively small dataset. So:\n",
        "\n",
        "\t•\tThere’s no pretrained model to fine-tune in this case\n",
        "\t•\tWe train the entire LSTM end-to-end from the Nottingham melodies\n",
        "\t•\t“Fine-tuning” usually refers to taking a pretrained model (like GPT, BERT, MusicGen, etc.) and adapting it to a new task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dqST-k0xvgAo"
      },
      "outputs": [],
      "source": [
        "# STEP 4: Define LSTM model\n",
        "import torch.nn as nn\n",
        "\n",
        "class NextNoteLSTM(nn.Module):\n",
        "    def __init__(self, note_vocab):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(note_vocab, 32)\n",
        "        self.lstm = nn.LSTM(32, 64, batch_first=True)\n",
        "        self.fc = nn.Linear(64, note_vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        return self.fc(x[:, -1, :])  # predict only the next note\n",
        "\n",
        "model = NextNoteLSTM(len(all_notes))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOYChvPbtpRb"
      },
      "source": [
        "### The current features are:\n",
        "\t•\tA sequence of 8 previous notes\n",
        "\t•\tEncoded as integers using LabelEncoder\n",
        "\t•\tEach note is one of the 21 unique note names (C, F#, A-, etc.)\n",
        "\n",
        "### Why these features?\n",
        "\t•\tThe previous notes define musical context\n",
        "\t•\tMusic has strong sequential structure (e.g., scales, motifs, repetition)\n",
        "\t•\tAn LSTM can learn patterns like “C → D → E → F → probably G”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe7IFqANvkNW",
        "outputId": "d785be06-84dc-464b-d8a4-e0acc638e2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Avg Loss: 1.7527 - Accuracy: 0.3490\n",
            "Epoch 2 - Avg Loss: 1.6901 - Accuracy: 0.3626\n",
            "Epoch 3 - Avg Loss: 1.6647 - Accuracy: 0.3706\n",
            "Epoch 4 - Avg Loss: 1.6460 - Accuracy: 0.3764\n",
            "Epoch 5 - Avg Loss: 1.6299 - Accuracy: 0.3812\n",
            "Epoch 6 - Avg Loss: 1.6187 - Accuracy: 0.3853\n",
            "Epoch 7 - Avg Loss: 1.6063 - Accuracy: 0.3889\n",
            "Epoch 8 - Avg Loss: 1.5948 - Accuracy: 0.3922\n",
            "Epoch 9 - Avg Loss: 1.5872 - Accuracy: 0.3950\n",
            "Epoch 10 - Avg Loss: 1.5787 - Accuracy: 0.3976\n",
            "Epoch 11 - Avg Loss: 1.5685 - Accuracy: 0.4001\n",
            "Epoch 12 - Avg Loss: 1.5639 - Accuracy: 0.4024\n",
            "Epoch 13 - Avg Loss: 1.5602 - Accuracy: 0.4044\n",
            "Epoch 14 - Avg Loss: 1.5558 - Accuracy: 0.4064\n",
            "Epoch 15 - Avg Loss: 1.5481 - Accuracy: 0.4082\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# STEP 5: Train the model\n",
        "for epoch in range(15):\n",
        "    total_loss = 0\n",
        "    for x, y in loader:\n",
        "        opt.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "        # Calculate accuracy\n",
        "        preds = out.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f} - Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsVaVdqcsavA",
        "outputId": "46957adf-568c-434f-e089-84819de59c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique notes: 21\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of unique notes:\", len(all_notes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcXfd2O9sdOI",
        "outputId": "ba833677-a831-439f-8d96-2ac519d4169b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'A#',\n",
              " 'A-',\n",
              " 'B',\n",
              " 'B#',\n",
              " 'B-',\n",
              " 'C',\n",
              " 'C#',\n",
              " 'C-',\n",
              " 'D',\n",
              " 'D#',\n",
              " 'D-',\n",
              " 'E',\n",
              " 'E#',\n",
              " 'E-',\n",
              " 'F',\n",
              " 'F#',\n",
              " 'F-',\n",
              " 'G',\n",
              " 'G#',\n",
              " 'G-']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "all_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_7d0f1iQvoBU"
      },
      "outputs": [],
      "source": [
        "# STEP 6: Generate 3 variations of melody\n",
        "from music21 import stream, note as m21note\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "for i in range(1, 4):\n",
        "    with torch.no_grad():\n",
        "        start = random.choice(input_seqs)\n",
        "        generated = list(start)\n",
        "        for _ in range(32):\n",
        "            input_tensor = torch.tensor([generated[-seq_length:]]).long()\n",
        "            probs = F.softmax(model(input_tensor), dim=-1)\n",
        "            next_note = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated.append(next_note)\n",
        "        decoded = note_encoder.inverse_transform(generated)\n",
        "\n",
        "        # Save each version\n",
        "        s = stream.Stream()\n",
        "        for n in decoded:\n",
        "            s.append(m21note.Note(n))\n",
        "        s.write('midi', fp=f'symbolic_conditioned_{i}.mid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_vtCzdZTtC1V"
      },
      "outputs": [],
      "source": [
        "# !pip install fluidsynth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "zlADJpHKu4uf",
        "outputId": "ed439452-6fa6-4a0c-8929-785b53027681"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fluidsynth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8c1977453869>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfluidsynth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert MIDI to audio using the fluidsynth command-line tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Ensure the output directory exists if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fluidsynth'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import fluidsynth\n",
        "import os\n",
        "\n",
        "# Convert MIDI to audio using the fluidsynth command-line tool\n",
        "# Ensure the output directory exists if needed\n",
        "output_dir = \"/content\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "midi_file = \"/content/symbolic_conditioned_1.mid\"\n",
        "output_wav = os.path.join(output_dir, \"output.wav\")\n",
        "soundfont_path = \"/usr/share/sounds/sf2/FluidR3_GM.sf2\"  # default SoundFont\n",
        "\n",
        "command = f\"fluidsynth -ni -a alsa -m alsa_seq {soundfont_path} {midi_file} -F {output_wav}\"\n",
        "\n",
        "# Execute the command\n",
        "# The '!' prefix in a Jupyter notebook executes the command in the shell\n",
        "!{command}\n",
        "\n",
        "# Play the result\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Correct playback from file\n",
        "ipd.Audio(filename=\"/content/output.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u6L2JJ-TAaY"
      },
      "outputs": [],
      "source": [
        "midi_file = \"/content/symbolic_conditioned_2.mid\"\n",
        "\n",
        "# Execute the command\n",
        "# The '!' prefix in a Jupyter notebook executes the command in the shell\n",
        "!{command}\n",
        "\n",
        "# Play the result\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Correct playback from file\n",
        "ipd.Audio(filename=\"/content/output.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMJWxtdcTJ2C"
      },
      "outputs": [],
      "source": [
        "midi_file = \"/content/symbolic_conditioned_3.mid\"\n",
        "\n",
        "# Execute the command\n",
        "# The '!' prefix in a Jupyter notebook executes the command in the shell\n",
        "!{command}\n",
        "\n",
        "# Play the result\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Correct playback from file\n",
        "ipd.Audio(filename=\"/content/output.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wMwkj7a11tT"
      },
      "source": [
        "### Add All Useful Features\n",
        "\n",
        "From each note:\n",
        "\n",
        "\t•\tPitch height: note.pitch.midi\n",
        "\t•\tRhythmic duration: note.quarterLength\n",
        "\t•\tInterval: difference between pitch[i] and pitch[i-1]\n",
        "\t•\t(Optional) Key: use score.analyze('key')\n",
        "\t•\t(Optional) Time signature: extract from score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNB-1QG4vpbN"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Parse melody for pitch, duration, and interval features\n",
        "import os\n",
        "from music21 import converter, note\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Parse melodies and extract features\n",
        "def parse_feature_sequences(folder_path, max_files=50):\n",
        "    pitch_seqs = []\n",
        "    duration_seqs = []\n",
        "    interval_seqs = []\n",
        "    file_list = sorted([f for f in os.listdir(folder_path) if f.endswith('.abc')])[:max_files]\n",
        "    for file in file_list:\n",
        "        try:\n",
        "            score = converter.parse(os.path.join(folder_path, file))\n",
        "            pitches = []\n",
        "            durations = []\n",
        "            for el in score.flat.notes:\n",
        "                if isinstance(el, note.Note):\n",
        "                    # Ensure pitch is an integer\n",
        "                    pitches.append(int(el.pitch.midi))\n",
        "                    durations.append(el.quarterLength)\n",
        "\n",
        "            if len(pitches) > 10:\n",
        "                intervals = [0] + [pitches[i+1] - pitches[i] for i in range(len(pitches)-1)]\n",
        "                pitch_seqs.append(pitches)\n",
        "                duration_seqs.append(durations)\n",
        "                interval_seqs.append(intervals)\n",
        "        except:\n",
        "            continue\n",
        "    return pitch_seqs, duration_seqs, interval_seqs\n",
        "\n",
        "dataset_path = \"/content/nottingham-dataset/ABC_cleaned\"\n",
        "pitch_seqs, duration_seqs, interval_seqs = parse_feature_sequences(dataset_path)\n",
        "\n",
        "# Create a new LabelEncoder specifically for the unique MIDI pitch numbers\n",
        "all_pitches = sorted(list(set(p for seq in pitch_seqs for p in seq)))\n",
        "pitch_class_encoder = LabelEncoder().fit(all_pitches)\n",
        "\n",
        "\n",
        "# Normalize durations and intervals\n",
        "scaler_dur = MinMaxScaler()\n",
        "scaler_int = MinMaxScaler()\n",
        "# Flatten lists of lists into a single list for fitting the scalers\n",
        "flat_durations = np.array([item for sublist in duration_seqs for item in sublist]).reshape(-1, 1)\n",
        "flat_intervals = np.array([item for sublist in interval_seqs for item in sublist]).reshape(-1, 1)\n",
        "\n",
        "scaler_dur.fit(flat_durations)\n",
        "scaler_int.fit(flat_intervals)\n",
        "\n",
        "norm_duration_seqs = [scaler_dur.transform(np.array(seq).reshape(-1, 1)).flatten() for seq in duration_seqs]\n",
        "norm_interval_seqs = [scaler_int.transform(np.array(seq).reshape(-1, 1)).flatten() for seq in interval_seqs]\n",
        "\n",
        "# Create (input, target) pairs for all 3 features\n",
        "seq_length = 8\n",
        "input_features, target_classes = [], []\n",
        "for pitch_seq, dur_seq, int_seq in zip(pitch_seqs, norm_duration_seqs, norm_interval_seqs):\n",
        "    for i in range(len(pitch_seq) - seq_length):\n",
        "        pitch_window = pitch_seq[i:i+seq_length]\n",
        "        dur_window = dur_seq[i:i+seq_length]\n",
        "        int_window = int_seq[i:i+seq_length]\n",
        "        # Ensure pitch data is float32 for input tensor\n",
        "        combined = np.stack([pitch_window, dur_window, int_window], axis=1).astype(np.float32)  # shape (seq_len, 3)\n",
        "        input_features.append(combined)\n",
        "        # Use the new pitch_class_encoder for the target\n",
        "        target_class = pitch_class_encoder.transform([pitch_seq[i+seq_length]])[0]\n",
        "        target_classes.append(target_class)\n",
        "\n",
        "class MelodyFeatureDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        # Inputs are already float32 from numpy array\n",
        "        return torch.tensor(self.inputs[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.long)\n",
        "\n",
        "# Use the new target_classes list for the dataset\n",
        "loader = DataLoader(MelodyFeatureDataset(input_features, target_classes), batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rd5QCyV1do8"
      },
      "outputs": [],
      "source": [
        "# STEP 4: Define LSTM model with feature input\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeatureLSTM(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=3, hidden_size=64, batch_first=True)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        return self.fc(hn[-1])  # output shape: (batch_size, num_classes)\n",
        "\n",
        "num_classes = len(pitch_class_encoder.classes_)\n",
        "model = FeatureLSTM(num_classes=num_classes) # Re-instantiate model with correct output size\n",
        "# You would typically load trained weights here if you stopped and restarted the kernel\n",
        "# model.load_state_dict(torch.load('model_weights.pth')) # Example loading\n",
        "\n",
        "# Re-define loss and optimizer if you are re-running the script from here\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4QmYc9G19Ng"
      },
      "outputs": [],
      "source": [
        "# STEP 5: Train the model\n",
        "for epoch in range(15):\n",
        "    total_loss = 0\n",
        "    for x, y in loader:\n",
        "        opt.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i_4Lh5a2QZ5"
      },
      "outputs": [],
      "source": [
        "# STEP 6: Generate new melody\n",
        "from music21 import stream, note as m21note\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Select a random starting sequence of features\n",
        "    start_idx = np.random.randint(len(input_features))\n",
        "    seed_features = torch.tensor(input_features[start_idx], dtype=torch.float32).unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    # Initialize generated sequence with the starting pitch values (not encoded classes)\n",
        "    generated_pitches = list(seed_features[0, :, 0].numpy().astype(int)) # Get raw pitches from seed\n",
        "\n",
        "    # Use the seed features as the initial input to the model\n",
        "    current_input_seq = seed_features.clone()\n",
        "\n",
        "    for _ in range(32): # Generate 32 new notes\n",
        "        # Model predicts the probability distribution over pitch classes for the *next* note\n",
        "        # The input to the model is the sequence of (pitch, duration, interval) features\n",
        "        output_logits = model(current_input_seq) # Shape: (batch_size, num_classes)\n",
        "        probs = F.softmax(output_logits, dim=-1)\n",
        "\n",
        "        # Sample the next pitch class from the predicted distribution\n",
        "        predicted_pitch_class_tensor = torch.multinomial(probs, num_samples=1)\n",
        "        predicted_pitch_class = predicted_pitch_class_tensor.item()\n",
        "\n",
        "        # Decode the predicted pitch class back to a MIDI pitch number\n",
        "        next_pitch = pitch_class_encoder.inverse_transform([predicted_pitch_class])[0]\n",
        "\n",
        "        # For generation, we need to make up the duration and interval for the next note.\n",
        "        # A simple approach is to reuse the duration and interval of the last note in the input sequence.\n",
        "        # A more complex approach could involve a separate model or sampling strategy for duration and interval.\n",
        "        last_note_features = current_input_seq[0, -1, :] # Get features of the last note in the sequence\n",
        "        # The duration and interval features are already normalized in the input sequence\n",
        "        next_dur_norm = last_note_features[1].item()\n",
        "        next_int_norm = last_note_features[2].item() # This interval is between the last two notes, not the next one\n",
        "\n",
        "        # A better interval estimate would be the difference between the *new* pitch\n",
        "        # and the last pitch in the sequence.\n",
        "        last_pitch_raw = current_input_seq[0, -1, 0].item() # Get the raw pitch of the last note\n",
        "        next_interval_raw = next_pitch - last_pitch_raw\n",
        "        # Normalize this calculated interval\n",
        "        next_int_norm = scaler_int.transform([[next_interval_raw]])[0, 0]\n",
        "\n",
        "\n",
        "        # Create the feature vector for the next note (pitch, duration, interval)\n",
        "        # The pitch is the decoded pitch from the model prediction\n",
        "        # The duration and interval are derived (e.g., copied from the last note or calculated)\n",
        "        next_features = torch.tensor([[next_pitch, next_dur_norm, next_int_norm]], dtype=torch.float32) # Shape: (1, 3)\n",
        "\n",
        "        # Append the new features to the input sequence and drop the oldest\n",
        "        # The input sequence needs to maintain the shape (batch_size, seq_length, num_features)\n",
        "        current_input_seq = torch.cat((current_input_seq[:, 1:, :], next_features.unsqueeze(0)), dim=1)\n",
        "\n",
        "        # Add the new raw pitch to the generated list\n",
        "        generated_pitches.append(next_pitch)\n",
        "\n",
        "\n",
        "# STEP 7: Save melody to MIDI\n",
        "s = stream.Stream()\n",
        "for p in generated_pitches:\n",
        "    # music21 note.Note takes a MIDI pitch number\n",
        "    s.append(m21note.Note(int(round(p)))) # Round to the nearest integer just in case\n",
        "\n",
        "# You might want to add durations to the notes when saving the MIDI file\n",
        "# This would require storing generated durations as well.\n",
        "# For now, the default duration will be used by music21.\n",
        "\n",
        "s.write('midi', fp='symbolic_conditioned_with_features.mid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfvJFXPT8BV3"
      },
      "outputs": [],
      "source": [
        "import fluidsynth\n",
        "import os\n",
        "\n",
        "# Convert MIDI to audio using the fluidsynth command-line tool\n",
        "# Ensure the output directory exists if needed\n",
        "output_dir = \"/content\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "midi_file = \"/content/symbolic_conditioned_with_features.mid\"\n",
        "output_wav = os.path.join(output_dir, \"output.wav\")\n",
        "soundfont_path = \"/usr/share/sounds/sf2/FluidR3_GM.sf2\"  # default SoundFont\n",
        "\n",
        "command = f\"fluidsynth -ni -a alsa -m alsa_seq {soundfont_path} {midi_file} -F {output_wav}\"\n",
        "\n",
        "# Execute the command\n",
        "# The '!' prefix in a Jupyter notebook executes the command in the shell\n",
        "!{command}\n",
        "\n",
        "# Play the result\n",
        "import IPython.display as ipd\n",
        "ipd.Audio(output_wav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kno0hEWL_YOc"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Parse melody for pitch, duration, and interval features\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from music21 import converter, note\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# Parse melodies and extract features\n",
        "def parse_feature_sequences(folder_path, max_files=50):\n",
        "    pitch_seqs, duration_seqs, interval_seqs = [], [], []\n",
        "    file_list = sorted([f for f in os.listdir(folder_path) if f.endswith('.abc')])[:max_files]\n",
        "    for file in file_list:\n",
        "        try:\n",
        "            score = converter.parse(os.path.join(folder_path, file))\n",
        "            pitches, durations = [], []\n",
        "            for el in score.flat.notes:\n",
        "                if isinstance(el, note.Note):\n",
        "                    pitches.append(el.pitch.nameWithOctave)\n",
        "                    durations.append(el.quarterLength)\n",
        "            if len(pitches) > 10:\n",
        "                midi_vals = [note.Note(p).pitch.midi for p in pitches]\n",
        "                intervals = [0] + [midi_vals[i+1] - midi_vals[i] for i in range(len(midi_vals)-1)]\n",
        "                pitch_seqs.append(pitches)\n",
        "                duration_seqs.append(durations)\n",
        "                interval_seqs.append(intervals)\n",
        "        except:\n",
        "            continue\n",
        "    return pitch_seqs, duration_seqs, interval_seqs\n",
        "\n",
        "def build_dataset(pitch_seqs, duration_seqs, interval_seqs, features, seq_length=8):\n",
        "    note_encoder = LabelEncoder().fit([p for seq in pitch_seqs for p in seq])\n",
        "    scaler_dur = MinMaxScaler()\n",
        "    scaler_int = MinMaxScaler()\n",
        "    flat_durs = np.concatenate(duration_seqs).reshape(-1, 1)\n",
        "    flat_ints = np.concatenate(interval_seqs).reshape(-1, 1)\n",
        "    scaler_dur.fit(flat_durs)\n",
        "    scaler_int.fit(flat_ints)\n",
        "\n",
        "    input_features, target_classes = [], []\n",
        "    for p_seq, d_seq, i_seq in zip(pitch_seqs, duration_seqs, interval_seqs):\n",
        "        if len(p_seq) <= seq_length:\n",
        "            continue\n",
        "        p_encoded = note_encoder.transform(p_seq)\n",
        "        d_scaled = scaler_dur.transform(np.array(d_seq).reshape(-1, 1)).flatten()\n",
        "        i_scaled = scaler_int.transform(np.array(i_seq).reshape(-1, 1)).flatten()\n",
        "\n",
        "        for i in range(len(p_seq) - seq_length):\n",
        "            window_feats = []\n",
        "            if 'pitch' in features:\n",
        "                window_feats.append(p_encoded[i:i+seq_length])\n",
        "            if 'duration' in features:\n",
        "                window_feats.append(d_scaled[i:i+seq_length])\n",
        "            if 'interval' in features:\n",
        "                window_feats.append(i_scaled[i:i+seq_length])\n",
        "\n",
        "            combined = np.stack(window_feats, axis=1)\n",
        "            input_features.append(combined)\n",
        "            target_classes.append(p_encoded[i+seq_length])\n",
        "\n",
        "    return input_features, target_classes, note_encoder\n",
        "\n",
        "class MelodyDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.inputs[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.long)\n",
        "\n",
        "import torch.nn as nn\n",
        "class FeatureLSTM(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, 128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, output_size)\n",
        "    def forward(self, x):\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        return self.fc(hn[-1])\n",
        "\n",
        "# STEP 4: Experiment loop\n",
        "pitch_seqs, dur_seqs, int_seqs = parse_feature_sequences(\"/content/nottingham-dataset/ABC_cleaned\")\n",
        "feature_sets = {\n",
        "    \"pitch_only\": ['pitch'],\n",
        "    \"pitch_duration\": ['pitch', 'duration'],\n",
        "    \"pitch_interval\": ['pitch', 'interval'],\n",
        "    \"all\": ['pitch', 'duration', 'interval']\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, feats in feature_sets.items():\n",
        "    print(f\"\\nTraining with features: {feats}\")\n",
        "    X, y, encoder = build_dataset(pitch_seqs, dur_seqs, int_seqs, feats)\n",
        "    dataset = MelodyDataset(X, y)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    model = FeatureLSTM(input_size=len(feats), output_size=len(encoder.classes_))\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        total_loss = 0\n",
        "        for xb, yb in loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = loss_fn(out, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
        "    results.append({\"features\": feats, \"final_loss\": avg_loss})\n",
        "\n",
        "# STEP 5: Show results\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfb-rhFN7059"
      },
      "source": [
        "Add Valdation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKzOZAuS70pn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# STEP 4: Experiment loop with validation\n",
        "results = []\n",
        "for name, feats in feature_sets.items():\n",
        "    print(f\"\\nTraining with features: {feats}\")\n",
        "    X, y, encoder = build_dataset(pitch_seqs, dur_seqs, int_seqs, feats)\n",
        "\n",
        "    # Add validation split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    train_loader = DataLoader(MelodyDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(MelodyDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
        "\n",
        "    model = FeatureLSTM(input_size=len(feats), output_size=len(encoder.classes_))\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    for epoch in range(15):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = loss_fn(out, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                out = model(xb)\n",
        "                loss = loss_fn(out, yb)\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.argmax(out, dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    results.append({\"features\": feats, \"val_loss\": avg_val_loss, \"val_acc\": val_acc})\n",
        "\n",
        "# STEP 5: Show results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nAblation Results with Validation:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ3AZgpCEiFj"
      },
      "source": [
        "### train with xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8vsscj4KuXb"
      },
      "outputs": [],
      "source": [
        "# # STEP 1: Install dependencies (in Colab)\n",
        "# !pip install music21 xgboost scikit-learn pandas optuna --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWsDYFDSEgrU"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Extract enriched structured features for tuning\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from music21 import converter, note, key, meter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import optuna\n",
        "\n",
        "# Extract enhanced features\n",
        "# note_encoder is initialized outside the function so it can be accessed later for decoding\n",
        "note_encoder = LabelEncoder()\n",
        "def extract_enriched_features(folder_path, max_files=50, seq_length=8):\n",
        "    X = []\n",
        "    all_next_notes = []\n",
        "\n",
        "    file_list = sorted([f for f in os.listdir(folder_path) if f.endswith('.abc')])[:max_files]\n",
        "    for file in file_list:\n",
        "        try:\n",
        "            score = converter.parse(os.path.join(folder_path, file))\n",
        "            key_sig = score.analyze('key').tonic.name if score.analyze('key') else 'C'\n",
        "            time_sigs = list(score.recurse().getElementsByClass(meter.TimeSignature))\n",
        "            time_sig = str(time_sigs[0].ratioString) if time_sigs else \"4/4\"\n",
        "\n",
        "            parts = [n for n in score.flat.notes if isinstance(n, note.Note)]\n",
        "            if len(parts) <= seq_length:\n",
        "                continue\n",
        "\n",
        "            for i in range(len(parts) - seq_length):\n",
        "                window = parts[i:i+seq_length]\n",
        "                next_note = parts[i+seq_length]\n",
        "                pitches = [n.pitch.midi for n in window]\n",
        "                octaves = [n.pitch.octave for n in window]\n",
        "                durations = [n.quarterLength for n in window]\n",
        "                intervals = [0] + [pitches[j+1] - pitches[j] for j in range(len(pitches)-1)]\n",
        "                offsets = [n.offset % 4.0 for n in window]  # rough bar position\n",
        "\n",
        "                features = {\n",
        "                    \"last_pitch\": pitches[-1],\n",
        "                    \"mean_pitch\": np.mean(pitches),\n",
        "                    \"std_pitch\": np.std(pitches),\n",
        "                    \"last_octave\": octaves[-1],\n",
        "                    \"mean_octave\": np.mean(octaves),\n",
        "                    \"mean_duration\": np.mean(durations),\n",
        "                    \"std_duration\": np.std(durations),\n",
        "                    \"mean_interval\": np.mean(intervals),\n",
        "                    \"std_interval\": np.std(intervals),\n",
        "                    \"mean_offset\": np.mean(offsets),\n",
        "                    \"key\": key_sig,\n",
        "                    \"time_sig\": time_sig\n",
        "                }\n",
        "                X.append(features)\n",
        "                all_next_notes.append(next_note.nameWithOctave)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Encode the target variable (next notes) here, inside the function\n",
        "    y_encoded = note_encoder.fit_transform(all_next_notes)\n",
        "    df = pd.DataFrame(X)\n",
        "    df = pd.get_dummies(df)  # One-hot encode categorical features like key/time_sig\n",
        "    # Ensure column names are valid for XGBoost (replace problematic characters)\n",
        "    df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n",
        "\n",
        "    return df, y_encoded # Return the correctly encoded target\n",
        "\n",
        "# Call the function to get the features and the correctly encoded target variable\n",
        "X_df, y = extract_enriched_features(\"/content/nottingham-dataset/ABC_cleaned\")\n",
        "\n",
        "# y is now the correctly encoded target variable (starting from 0)\n",
        "# Remove the incorrect re-encoding and shifting lines:\n",
        "# y = note_encoder.fit_transform(y)  # already done - REMOVE THIS LINE\n",
        "# y = y - y.min()  # shift so labels start at 0 - REMOVE THIS LINE\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUAsAuiBPeHz"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WKvIqZjSkNr"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Extract enriched structured features for tuning\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from music21 import converter, note, key, meter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import optuna\n",
        "\n",
        "# Extract enhanced features\n",
        "def extract_enriched_features(folder_path, max_files=50, seq_length=8):\n",
        "    X = []\n",
        "    all_next_notes = []\n",
        "\n",
        "    file_list = sorted([f for f in os.listdir(folder_path) if f.endswith('.abc')])[:max_files]\n",
        "    for file in file_list:\n",
        "        try:\n",
        "            score = converter.parse(os.path.join(folder_path, file))\n",
        "            key_sig = score.analyze('key').tonic.name if score.analyze('key') else 'C'\n",
        "            time_sigs = list(score.recurse().getElementsByClass(meter.TimeSignature))\n",
        "            time_sig = str(time_sigs[0].ratioString) if time_sigs else \"4/4\"\n",
        "\n",
        "            parts = [n for n in score.flatten().notes if isinstance(n, note.Note)]\n",
        "            if len(parts) <= seq_length:\n",
        "                continue\n",
        "\n",
        "            for i in range(len(parts) - seq_length):\n",
        "                window = parts[i:i+seq_length]\n",
        "                next_note = parts[i+seq_length]\n",
        "                pitches = [n.pitch.midi for n in window]\n",
        "                octaves = [n.pitch.octave for n in window]\n",
        "                durations = [n.quarterLength for n in window]\n",
        "                intervals = [0] + [pitches[j+1] - pitches[j] for j in range(len(pitches)-1)]\n",
        "                offsets = [n.offset % 4.0 for n in window]  # rough bar position\n",
        "\n",
        "                features = {\n",
        "                    \"last_pitch\": pitches[-1],\n",
        "                    \"mean_pitch\": np.mean(pitches),\n",
        "                    \"std_pitch\": np.std(pitches),\n",
        "                    \"last_octave\": octaves[-1],\n",
        "                    \"mean_octave\": np.mean(octaves),\n",
        "                    \"mean_duration\": np.mean(durations),\n",
        "                    \"std_duration\": np.std(durations),\n",
        "                    \"mean_interval\": np.mean(intervals),\n",
        "                    \"std_interval\": np.std(intervals),\n",
        "                    \"mean_offset\": np.mean(offsets),\n",
        "                    \"key\": key_sig,\n",
        "                    \"time_sig\": time_sig\n",
        "                }\n",
        "                X.append(features)\n",
        "                all_next_notes.append(next_note.nameWithOctave)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(X)\n",
        "    df = pd.get_dummies(df)  # One-hot encode categorical features like key/time_sig\n",
        "\n",
        "    note_encoder = LabelEncoder()\n",
        "    encoded = note_encoder.fit_transform(all_next_notes)\n",
        "    used_classes = np.unique(encoded)\n",
        "    remap = {old: new for new, old in enumerate(used_classes)}\n",
        "    y = np.array([remap[val] for val in encoded])\n",
        "\n",
        "    print(\"✔️ Final y classes:\", np.unique(y))\n",
        "    return df, y\n",
        "\n",
        "# Load data and split\n",
        "X_df, y = extract_enriched_features(\"/content/nottingham-dataset/ABC_cleaned\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0WGfKK0FoIu"
      },
      "outputs": [],
      "source": [
        "# STEP 4: Optuna tuning with XGBoost\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, preds)\n",
        "    return acc\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"\\nBest trial:\")\n",
        "print(study.best_trial)\n",
        "print(\"\\nBest parameters:\")\n",
        "print(study.best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEbtBq4ae8Ol"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Install dependencies (in Colab)\n",
        "!pip install music21 xgboost scikit-learn pandas optuna --quiet\n",
        "\n",
        "# STEP 2: Clone Nottingham dataset\n",
        "!git clone https://github.com/jukedeck/nottingham-dataset.git\n",
        "\n",
        "# STEP 3: Extract enriched structured features for tuning\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from music21 import converter, note, key, meter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import optuna\n",
        "\n",
        "# Extract enhanced features\n",
        "def extract_enriched_features(folder_path, max_files=50, seq_length=8):\n",
        "    X = []\n",
        "    all_next_notes = []\n",
        "\n",
        "    file_list = sorted([f for f in os.listdir(folder_path) if f.endswith('.abc')])[:max_files]\n",
        "    for file in file_list:\n",
        "        try:\n",
        "            score = converter.parse(os.path.join(folder_path, file))\n",
        "            key_sig = score.analyze('key').tonic.name if score.analyze('key') else 'C'\n",
        "            time_sigs = list(score.recurse().getElementsByClass(meter.TimeSignature))\n",
        "            time_sig = str(time_sigs[0].ratioString) if time_sigs else \"4/4\"\n",
        "\n",
        "            parts = [n for n in score.flatten().notes if isinstance(n, note.Note)]\n",
        "            if len(parts) <= seq_length:\n",
        "                continue\n",
        "\n",
        "            for i in range(len(parts) - seq_length):\n",
        "                window = parts[i:i+seq_length]\n",
        "                next_note = parts[i+seq_length]\n",
        "                pitches = [n.pitch.midi for n in window]\n",
        "                octaves = [n.pitch.octave for n in window]\n",
        "                durations = [n.quarterLength for n in window]\n",
        "                intervals = [0] + [pitches[j+1] - pitches[j] for j in range(len(pitches)-1)]\n",
        "                offsets = [n.offset % 4.0 for n in window]  # rough bar position\n",
        "\n",
        "                features = {\n",
        "                    \"last_pitch\": pitches[-1],\n",
        "                    \"mean_pitch\": np.mean(pitches),\n",
        "                    \"std_pitch\": np.std(pitches),\n",
        "                    \"last_octave\": octaves[-1],\n",
        "                    \"mean_octave\": np.mean(octaves),\n",
        "                    \"mean_duration\": np.mean(durations),\n",
        "                    \"std_duration\": np.std(durations),\n",
        "                    \"mean_interval\": np.mean(intervals),\n",
        "                    \"std_interval\": np.std(intervals),\n",
        "                    \"mean_offset\": np.mean(offsets),\n",
        "                    \"key\": key_sig,\n",
        "                    \"time_sig\": time_sig\n",
        "                }\n",
        "                X.append(features)\n",
        "                all_next_notes.append(next_note.nameWithOctave)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(X)\n",
        "    df = pd.get_dummies(df)  # One-hot encode categorical features like key/time_sig\n",
        "\n",
        "    note_encoder = LabelEncoder()\n",
        "    encoded = note_encoder.fit_transform(all_next_notes)\n",
        "    used_classes = np.unique(encoded)\n",
        "    remap = {old: new for new, old in enumerate(used_classes)}\n",
        "    y = np.array([remap[val] for val in encoded])\n",
        "\n",
        "    print(\"✔️ Final y classes:\", np.unique(y))\n",
        "    return df, y\n",
        "\n",
        "# Load data and split\n",
        "X_df, y = extract_enriched_features(\"/content/nottingham-dataset/ABC_cleaned\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# STEP 4: Optuna tuning with XGBoost\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'use_label_encoder': False,\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, preds)\n",
        "    return acc\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"\\nBest trial:\")\n",
        "print(study.best_trial)\n",
        "print(\"\\nBest parameters:\")\n",
        "print(study.best_params)\n",
        "\n",
        "# Final model with best params\n",
        "best_model = XGBClassifier(**study.best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "best_model.fit(X_train, y_train)\n",
        "preds = best_model.predict(X_val)\n",
        "print(\"\\nFinal Validation Accuracy:\", accuracy_score(y_val, preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-8yz25WWtRh"
      },
      "outputs": [],
      "source": [
        "X_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hugjzs7xUIOj"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djxunTpwLgI4"
      },
      "outputs": [],
      "source": [
        "# Final model with best params\n",
        "best_model = XGBClassifier(**study.best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "best_model.fit(X_train, y_train)\n",
        "preds = best_model.predict(X_val)\n",
        "print(\"\\nFinal Validation Accuracy:\", accuracy_score(y_val, preds))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}