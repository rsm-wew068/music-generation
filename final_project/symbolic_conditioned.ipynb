{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# !rm -rf /content/*\n",
    "\n",
    "# # %%\n",
    "# import gc\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99339f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install music21 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jukedeck/nottingham-dataset.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86267da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19038df5",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 1 symbolic conditioned generation (Nottingham)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077988b7",
   "metadata": {},
   "source": [
    "## 1. Multi-instrument REMI modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db59ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from music21 import converter, note, chord\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f786ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def parse_multi_instrument_sequences(folder_path, max_files=14):\n",
    "    melody_seqs, chord_seqs, bass_seqs = [], [], []\n",
    "    files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".abc\")])\n",
    "    print(f\"Found {len(files)} files. Parsing up to {max_files} files...\")\n",
    "\n",
    "    for f in files:\n",
    "        if len(melody_seqs) >= max_files:\n",
    "            break\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            score = converter.parse(os.path.join(folder_path, f))\n",
    "            if time.time() - start_time > 20:\n",
    "                print(f\"⏱️ Skipping {f} — parsing took too long\")\n",
    "                continue\n",
    "\n",
    "            melody = [n.nameWithOctave for n in score.flat.notes if isinstance(n, note.Note)]\n",
    "            chords = [c.root().name for c in score.chordify().flat.getElementsByClass('Chord')]\n",
    "            bass = [c.bass().nameWithOctave if c.bass() else c.root().nameWithOctave\n",
    "                    for c in score.chordify().flat.getElementsByClass('Chord')]\n",
    "\n",
    "            min_len = min(len(melody), len(chords), len(bass))\n",
    "            if min_len >= 16:\n",
    "                melody_seqs.append(melody[:min_len])\n",
    "                chord_seqs.append(chords[:min_len])\n",
    "                bass_seqs.append(bass[:min_len])\n",
    "            else:\n",
    "                print(f\"⚠️ {f} skipped: not enough notes\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error parsing {f}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"✅ Parsed {len(melody_seqs)} sequences.\")\n",
    "    return melody_seqs, chord_seqs, bass_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a751ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "melody_seqs, chord_seqs, bass_seqs = parse_multi_instrument_sequences(\"/content/nottingham-dataset/ABC_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dbbc0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Data augmentation: transpose sequences up/down 1 or 2 semitones\n",
    "from music21 import pitch\n",
    "def transpose_sequence(seq, semitones):\n",
    "    transposed = []\n",
    "    for item in seq:\n",
    "        try:\n",
    "            transposed.append(pitch.Pitch(item).transpose(semitones).nameWithOctave)\n",
    "        except Exception:\n",
    "            transposed.append(item)\n",
    "    return transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fd0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_melody, aug_chords, aug_bass = [], [], []\n",
    "for m_seq, c_seq, b_seq in zip(melody_seqs, chord_seqs, bass_seqs):\n",
    "    for st in [-2, -1, 1, 2]:\n",
    "        aug_melody.append(transpose_sequence(m_seq, st))\n",
    "        aug_chords.append([pitch.Pitch(ch + '4').transpose(st).name for ch in c_seq])\n",
    "        aug_bass.append(transpose_sequence(b_seq, st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f080085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original + augmented\n",
    "melody_seqs += aug_melody\n",
    "chord_seqs += aug_chords\n",
    "bass_seqs += aug_bass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2b183",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"✅ After augmentation: {len(melody_seqs)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662730a",
   "metadata": {},
   "source": [
    "## 2. REMI token modeling and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bcf7e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_token_sequence(melody_seqs, chord_seqs, bass_seqs):\n",
    "    sequences = []\n",
    "    for melody, chords, basses in zip(melody_seqs, chord_seqs, bass_seqs):\n",
    "        tokens = []\n",
    "        tokens.append(\"Bar_0\")\n",
    "        for i, (mel, ch, ba) in enumerate(zip(melody, chords, basses)):\n",
    "            tokens.append(f\"Position_{i}\")\n",
    "            tokens.append(f\"Track_Chords\")\n",
    "            tokens.append(f\"Chord_{ch}\")\n",
    "            tokens.append(f\"Track_Melody\")\n",
    "            tokens.append(f\"Note_{mel}\")\n",
    "            tokens.append(f\"Track_Bass\")\n",
    "            tokens.append(f\"Note_{ba}\")\n",
    "        sequences.append(tokens)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_seqs = build_token_sequence(melody_seqs, chord_seqs, bass_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "flat_tokens = [tok for seq in token_seqs for tok in seq]\n",
    "token_counts = Counter(flat_tokens)\n",
    "vocab = {tok: idx for idx, tok in enumerate(sorted(token_counts))}\n",
    "inv_vocab = {idx: tok for tok, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e711c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_token_seqs = [[vocab[tok] for tok in seq] for seq in token_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373173e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"Total unique tokens: {len(vocab)}\")\n",
    "print(f\"Example tokenized sequence: {encoded_token_seqs[0][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0ba6e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, token_seqs, seq_len=64):\n",
    "        self.seq_len = seq_len\n",
    "        self.samples = []\n",
    "        for seq in token_seqs:\n",
    "            if len(seq) > seq_len:\n",
    "                for i in range(len(seq) - seq_len):\n",
    "                    self.samples.append((\n",
    "                        torch.tensor(seq[i:i+seq_len], dtype=torch.long),\n",
    "                        torch.tensor(seq[i+1:i+seq_len+1], dtype=torch.long)\n",
    "                    ))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a744b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug shortcut\n",
    "remi_dataset = TokenDataset(encoded_token_seqs, seq_len=64)\n",
    "remi_dataset.samples = remi_dataset.samples[:2000]\n",
    "remi_loader = DataLoader(remi_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37094d23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 512, emb_size))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead)\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(emb_size, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = x.transpose(0, 1)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(x.size(0)).to(x.device)\n",
    "        memory = torch.zeros_like(x)\n",
    "        out = self.transformer(x, memory, tgt_mask=tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = MiniTransformer(vocab_size=len(vocab)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "\n",
    "# === Original Transformer Training Loop (baseline, small subset) ===\n",
    "transformer_model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for x, y in remi_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = transformer_model(x)\n",
    "        loss = criterion(out.view(-1, out.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss / len(remi_loader):.4f}\")\n",
    "\n",
    "# === Fine-tuned Transformer (longer training, full data, LR scheduler) ===\n",
    "# Remove the 2k-sample limit and use full data with validation, LR scheduling, and gradient clipping\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Reconstruct full dataset (remi_dataset.samples may have been truncated above)\n",
    "remi_dataset_full = TokenDataset(encoded_token_seqs, seq_len=64)\n",
    "# Split into train/val\n",
    "train_samples, val_samples = train_test_split(remi_dataset_full.samples, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_samples, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_samples, batch_size=8)\n",
    "\n",
    "transformer_model.train()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = transformer_model(x)\n",
    "        loss = criterion(out.view(-1, out.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    # Optional: validation loss\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = transformer_model(x)\n",
    "            loss = criterion(out.view(-1, out.size(-1)), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "    print(f\"[Fine-tune] Epoch {epoch+1} - Train Loss: {total_loss / len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")\n",
    "torch.save(transformer_model.state_dict(), \"fine_tuned_transformer_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04788f",
   "metadata": {},
   "source": [
    "## 3. Feature-based modeling (pitch, duration, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ca7c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def parse_feature_sequences(folder_path, max_files=14):\n",
    "    pitch_seqs, duration_seqs, interval_seqs = [], [], []\n",
    "    files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".abc\")])[:max_files]\n",
    "    print(f\"Found {len(files)} files. Parsing up to {max_files}...\")\n",
    "\n",
    "    for f in files:\n",
    "        if len(pitch_seqs) >= max_files:\n",
    "            break\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            score = converter.parse(os.path.join(folder_path, f))\n",
    "            if time.time() - start_time > 20:\n",
    "                print(f\"⏱️ Skipping {f} — parsing took too long\")\n",
    "                continue\n",
    "\n",
    "            pitches, durations = [], []\n",
    "            for el in score.flat.notes:\n",
    "                if isinstance(el, note.Note):\n",
    "                    pitches.append(el.pitch.nameWithOctave)\n",
    "                    durations.append(el.quarterLength)\n",
    "\n",
    "            if len(pitches) > 10:\n",
    "                midi_vals = [note.Note(p).pitch.midi for p in pitches]\n",
    "                intervals = [0] + [midi_vals[i+1] - midi_vals[i] for i in range(len(midi_vals)-1)]\n",
    "                pitch_seqs.append(pitches)\n",
    "                duration_seqs.append(durations)\n",
    "                interval_seqs.append(intervals)\n",
    "            else:\n",
    "                print(f\"⚠️ {f} skipped: not enough notes\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error parsing {f}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"✅ Parsed {len(pitch_seqs)} sequences.\")\n",
    "    return pitch_seqs, duration_seqs, interval_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad446c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_dataset(pitch_seqs, duration_seqs, interval_seqs, features, seq_length=8):\n",
    "    note_encoder = LabelEncoder().fit([p for seq in pitch_seqs for p in seq])\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler_dur = MinMaxScaler()\n",
    "    scaler_int = MinMaxScaler()\n",
    "    import numpy as np\n",
    "    flat_durs = np.concatenate(duration_seqs).reshape(-1, 1)\n",
    "    flat_ints = np.concatenate(interval_seqs).reshape(-1, 1)\n",
    "    scaler_dur.fit(flat_durs)\n",
    "    scaler_int.fit(flat_ints)\n",
    "    input_features, target_classes = [], []\n",
    "    for p_seq, d_seq, i_seq in zip(pitch_seqs, duration_seqs, interval_seqs):\n",
    "        if len(p_seq) <= seq_length:\n",
    "            continue\n",
    "        p_encoded = note_encoder.transform(p_seq)\n",
    "        d_scaled = scaler_dur.transform(np.array(d_seq).reshape(-1, 1)).flatten()\n",
    "        i_scaled = scaler_int.transform(np.array(i_seq).reshape(-1, 1)).flatten()\n",
    "        for i in range(len(p_seq) - seq_length):\n",
    "            window_feats = []\n",
    "            if 'pitch' in features:\n",
    "                window_feats.append(p_encoded[i:i+seq_length])\n",
    "            if 'duration' in features:\n",
    "                window_feats.append(d_scaled[i:i+seq_length])\n",
    "            if 'interval' in features:\n",
    "                window_feats.append(i_scaled[i:i+seq_length])\n",
    "            combined = np.stack(window_feats, axis=1)\n",
    "            input_features.append(combined)\n",
    "            target_classes.append(p_encoded[i+seq_length])\n",
    "    return input_features, target_classes, note_encoder, scaler_dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d34a6f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Prepare data for variable pitch/duration/interval modeling\n",
    "pitch_seqs, dur_seqs, int_seqs = parse_feature_sequences(\"/content/nottingham-dataset/ABC_cleaned\")\n",
    "input_features, _, note_encoder, scaler_dur = build_dataset(pitch_seqs, dur_seqs, int_seqs, features=['pitch', 'duration', 'interval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba464f7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65259fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # === Helper for decoding feature sequences to MIDI with duration quantization ===\n",
    "def quantize_duration(d):\n",
    "     \"\"\"Clamp and quantize duration for more natural pacing.\"\"\"\n",
    "     # Clamp duration to [0.5, 2.5] and quantize to common musical values\n",
    "     return min([0.5, 1.0, 1.5, 2.0, 3.0, 4.0], key=lambda x: abs(x - d))\n",
    "\n",
    "def decode_feature_sequence_to_midi(pitches, durations, filename=\"feature_output.mid\"):\n",
    "     \"\"\"Decodes a sequence of pitches and durations to a MIDI file, with quantized durations.\"\"\"\n",
    "     from music21 import stream, note as m21note\n",
    "     s = stream.Score()\n",
    "     p = stream.Part()\n",
    "     offset = 0.0\n",
    "     for pch, d in zip(pitches, durations):\n",
    "         n = m21note.Note(pch)\n",
    "         # Clamp and quantize duration for more natural pacing\n",
    "         d = max(0.5, min(d, 2.5))\n",
    "         n.quarterLength = quantize_duration(d)\n",
    "         n.offset = offset\n",
    "         p.append(n)\n",
    "         offset += n.quarterLength\n",
    "     s.append(p)\n",
    "     s.write('midi', fp=filename)\n",
    "     print(f\"Saved feature-based MIDI to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20528ed1",
   "metadata": {},
   "source": [
    "## 4. Symbolic Generation and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef1c51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_sequence(start_seq, model, vocab, inv_vocab, length=100):\n",
    "    model.eval()\n",
    "    generated = start_seq[:]\n",
    "    input_seq = torch.tensor(start_seq[-64:], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            out = model(input_seq)\n",
    "            next_token = torch.multinomial(F.softmax(out[0, -1], dim=-1), 1).item()\n",
    "            generated.append(next_token)\n",
    "            input_seq = torch.tensor(generated[-64:], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    return [inv_vocab[i] for i in generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffeb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "start_tokens = random.choice(encoded_token_seqs)[:64]\n",
    "generated_tokens = generate_sequence(start_tokens, transformer_model, vocab, inv_vocab, length=300)\n",
    "print(\"Generated tokens:\", generated_tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import note as m21note\n",
    "\n",
    "# Helper function for pitch shifting symbolic tokens\n",
    "def shift_note_token(token, semitone_shift):\n",
    "    if not token.startswith(\"Note_\"):\n",
    "        return token\n",
    "    try:\n",
    "        pitch_name = token.replace(\"Note_\", \"\")\n",
    "        p = m21note.Note(pitch_name).transpose(semitone_shift)\n",
    "        return f\"Note_{p.nameWithOctave}\"\n",
    "    except:\n",
    "        return token\n",
    "# Ensure correct transformer version to avoid read-only property errors in MusicGen\n",
    "import pkg_resources\n",
    "from subprocess import run\n",
    "\n",
    "def check_transformers_version(min_version=\"4.35.0\"):\n",
    "    try:\n",
    "        current = pkg_resources.get_distribution(\"transformers\").version\n",
    "        print(f\"✅ Transformers version: {current}\")\n",
    "        if pkg_resources.parse_version(current) < pkg_resources.parse_version(min_version):\n",
    "            print(f\"⚠️ Updating transformers to >= {min_version} to avoid config errors...\")\n",
    "            run([\"pip\", \"install\", f\"transformers>={min_version}\", \"--upgrade\"], check=True)\n",
    "            print(\"✅ Transformers updated successfully. Please restart the kernel.\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Transformers not found or version check failed. Installing latest version...\")\n",
    "        run([\"pip\", \"install\", \"transformers\", \"--upgrade\"], check=True)\n",
    "\n",
    "check_transformers_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14fd611",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# === Regenerate full-length (1-minute) version for Task 1 ===\n",
    "print(\"🎼 Regenerating 1-minute sequence for Task 1...\")\n",
    "start_tokens = random.choice(encoded_token_seqs)[:64]\n",
    "long_tokens = generate_sequence(start_tokens, transformer_model, vocab, inv_vocab, length=1000)\n",
    "# --- Patch symbolic generation for improved phrasing and realism ---\n",
    "# Split into beginning, middle, ending for musical structure\n",
    "beginning_tokens = long_tokens[:64]\n",
    "middle_tokens = long_tokens[64:512]\n",
    "ending_tokens = long_tokens[512:640]\n",
    "\n",
    "# Apply pitch shifting and thinning to middle/ending\n",
    "middle_tokens = [shift_note_token(tok, +5) for i, tok in enumerate(middle_tokens) if i % 4 != 0]\n",
    "ending_tokens = [shift_note_token(tok, -4) for tok in ending_tokens]\n",
    "\n",
    "# Stitch together\n",
    "long_tokens = beginning_tokens + middle_tokens + ending_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aecc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Structured symbolic generation for Task 1: intro → climax → resolution ===\n",
    "print(\"🎼 Generating structured musical arc for Task 1 (intro → climax → resolution)\")\n",
    "\n",
    "# Section 1: calm intro\n",
    "start_tokens = random.choice(encoded_token_seqs)[:64]\n",
    "intro_tokens = generate_sequence(start_tokens, transformer_model, vocab, inv_vocab, length=300)\n",
    "\n",
    "# Section 2: energetic middle (transpose notes up + remove every 5th token to thin)\n",
    "mid_start = intro_tokens[-64:]\n",
    "# The generate_sequence function returns tokens as strings like \"Note_A4\" or \"Chord_C\"\n",
    "# We need to convert these string tokens back to indices for the generate_sequence function\n",
    "# Use vocab.get(tok, 0) to handle potential missing tokens gracefully, though hopefully the vocab is complete\n",
    "mid_start_indices = [vocab.get(tok, 0) for tok in mid_start]\n",
    "middle_tokens = generate_sequence(mid_start_indices, transformer_model, vocab, inv_vocab, length=300)\n",
    "\n",
    "# Apply pitch shifting and thinning to middle section using the correct function\n",
    "# Only shift note tokens, keep others as they are\n",
    "# Use a list comprehension filtering out tokens that are not note tokens\n",
    "middle_tokens = [\n",
    "    shift_note_token(tok, +5) if tok.startswith(\"Note_\") else tok\n",
    "    for i, tok in enumerate(middle_tokens) if i % 4 != 0 # Thinning\n",
    "]\n",
    "\n",
    "\n",
    "# Section 3: resolution (transpose down)\n",
    "# Need to convert the string tokens from the middle_tokens back to indices for the generation function\n",
    "end_start = middle_tokens[-64:]\n",
    "end_start_indices = [vocab.get(tok, 0) for tok in end_start]\n",
    "ending_tokens = generate_sequence(end_start_indices, transformer_model, vocab, inv_vocab, length=300)\n",
    "\n",
    "# Apply pitch shifting to ending section using the correct function\n",
    "# Only shift note tokens, keep others as they are\n",
    "ending_tokens = [\n",
    "    shift_note_token(tok, -4) if tok.startswith(\"Note_\") else tok\n",
    "    for tok in ending_tokens\n",
    "]\n",
    "\n",
    "\n",
    "# Concatenate full arc\n",
    "full_tokens = intro_tokens + middle_tokens + ending_tokens\n",
    "\n",
    "# === Predict expressive duration with MLP ===\n",
    "\n",
    "# Extract features from REMI token sequences for duration prediction\n",
    "def extract_duration_features(token_seqs):\n",
    "    X_raw, y = [], []\n",
    "    for seq in token_seqs:\n",
    "        bar, pos, track = 0, 0, None\n",
    "        for i, tok in enumerate(seq):\n",
    "            if tok.startswith(\"Bar_\"):\n",
    "                bar = int(tok.split(\"_\")[1])\n",
    "            elif tok.startswith(\"Position_\"):\n",
    "                pos = int(tok.split(\"_\")[1])\n",
    "            elif tok.startswith(\"Track_\"):\n",
    "                track = tok.split(\"_\")[1]\n",
    "            elif tok.startswith(\"Note_\") and track:\n",
    "                pitch = tok.replace(\"Note_\", \"\")\n",
    "                X_raw.append([track, pos, pitch])\n",
    "                y.append(1.0 if track == \"Melody\" else 0.5)  # heuristic durations\n",
    "    return X_raw, y\n",
    "\n",
    "# Train a simple MLPRegressor to predict expressive duration\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "X_raw, y = extract_duration_features(token_seqs)\n",
    "encoder = OneHotEncoder(sparse_output=False).fit(X_raw)\n",
    "X = encoder.transform(X_raw)\n",
    "\n",
    "duration_model = MLPRegressor(hidden_layer_sizes=(64,), max_iter=1000)\n",
    "duration_model.fit(X, y)\n",
    "print(\"✅ Trained MLP to predict expressive duration.\")\n",
    "\n",
    "\n",
    "# === Predict expressive velocity with MLP ===\n",
    "def extract_velocity_features(token_seqs):\n",
    "    X_raw, y = [], []\n",
    "    for seq in token_seqs:\n",
    "        bar, pos, track = 0, 0, None\n",
    "        for tok in seq:\n",
    "            if tok.startswith(\"Bar_\"):\n",
    "                bar = int(tok.split(\"_\")[1])\n",
    "            elif tok.startswith(\"Position_\"):\n",
    "                pos = int(tok.split(\"_\")[1])\n",
    "            elif tok.startswith(\"Track_\"):\n",
    "                track = tok.split(\"_\")[1]\n",
    "            elif tok.startswith(\"Note_\") and track:\n",
    "                pitch = tok.replace(\"Note_\", \"\")\n",
    "                X_raw.append([track, pos, pitch])\n",
    "                y.append(80 if track == \"Melody\" else 60)  # Heuristic velocity\n",
    "    return X_raw, y\n",
    "\n",
    "Xv_raw, yv = extract_velocity_features(token_seqs)\n",
    "velocity_encoder = OneHotEncoder(sparse_output=False).fit(Xv_raw)\n",
    "Xv = velocity_encoder.transform(Xv_raw)\n",
    "\n",
    "velocity_model = MLPRegressor(hidden_layer_sizes=(64,), max_iter=1000)\n",
    "velocity_model.fit(Xv, yv)\n",
    "print(\"✅ Trained MLP to predict expressive velocity.\")\n",
    "\n",
    "# === Predict articulation with MLP ===\n",
    "def extract_articulation_features(token_seqs):\n",
    "    X_raw, y = [], []\n",
    "    for seq in token_seqs:\n",
    "        bar, pos, track = 0, 0, None\n",
    "        for tok in seq:\n",
    "            if tok.startswith(\"Bar_\"):\n",
    "                bar = int(tok.split(\"_\")[1])\n",
    "            elif tok.startswith(\"Position_\"):\n",
    "                pos = int(tok.split(\"_\")[1])\n",
    "            elif tok.startswith(\"Track_\"):\n",
    "                track = tok.split(\"_\")[1]\n",
    "            elif tok.startswith(\"Note_\") and track:\n",
    "                pitch = tok.replace(\"Note_\", \"\")\n",
    "                X_raw.append([track, pos, pitch])\n",
    "                y.append(1 if track == \"Melody\" else 0)  # Legato for melody, staccato otherwise\n",
    "    return X_raw, y\n",
    "\n",
    "Xa_raw, ya = extract_articulation_features(token_seqs)\n",
    "articulation_encoder = OneHotEncoder(sparse_output=False).fit(Xa_raw)\n",
    "Xa = articulation_encoder.transform(Xa_raw)\n",
    "\n",
    "articulation_model = MLPRegressor(hidden_layer_sizes=(32,), max_iter=1000)\n",
    "articulation_model.fit(Xa, ya)\n",
    "print(\"✅ Trained MLP to predict articulation (legato/staccato).\")\n",
    "\n",
    "# === Predict tempo curve (beats per minute) with MLP ===\n",
    "def extract_tempo_features(token_seqs):\n",
    "    X_raw, y = [], []\n",
    "    for seq in token_seqs:\n",
    "        bar, pos, track = 0, 0, None\n",
    "        for tok in seq:\n",
    "            if tok.startswith(\"Bar_\"):\n",
    "                bar = int(tok.split(\"_\")[1])\n",
    "            elif tok.startswith(\"Position_\"):\n",
    "                pos = int(tok.split(\"_\")[1])\n",
    "            elif tok.startswith(\"Track_\"):\n",
    "                track = tok.split(\"_\")[1]\n",
    "            elif tok.startswith(\"Note_\") and track:\n",
    "                pitch = tok.replace(\"Note_\", \"\")\n",
    "                X_raw.append([bar, pos])\n",
    "                y.append(60 + (bar % 4) * 10)  # Simulated tempo pattern\n",
    "    return X_raw, y\n",
    "\n",
    "Xt_raw, yt = extract_tempo_features(token_seqs)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tempo_scaler = StandardScaler()\n",
    "Xt = tempo_scaler.fit_transform(Xt_raw)\n",
    "\n",
    "tempo_model = MLPRegressor(hidden_layer_sizes=(32,), max_iter=1000)\n",
    "tempo_model.fit(Xt, yt)\n",
    "print(\"✅ Trained MLP to predict tempo curve.\")\n",
    "\n",
    "# Updated decode_to_midi with articulation prediction\n",
    "from music21 import stream, note as m21note, chord as m21chord, tempo as m21tempo\n",
    "def decode_to_midi(\n",
    "    tokens,\n",
    "    filename=\"multi_instrument_output.mid\",\n",
    "    duration_model=None,\n",
    "    encoder=None,\n",
    "    velocity_model=None,\n",
    "    velocity_encoder=None,\n",
    "    articulation_model=None,\n",
    "    articulation_encoder=None,\n",
    "    tempo_model=None\n",
    "):\n",
    "    s = stream.Score()\n",
    "    parts = {\"Melody\": stream.Part(), \"Chords\": stream.Part(), \"Bass\": stream.Part()}\n",
    "    # REMI-style bar/position time tracking\n",
    "    bar_num = 0\n",
    "    current_position = 0.0\n",
    "    steps_per_bar = 16\n",
    "    # Patch: double the duration per position for more realistic phrasing\n",
    "    quarter_length_per_step = (4.0 / steps_per_bar) * 2.0\n",
    "    track = None\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"Bar_\"):\n",
    "            bar_num = int(tok.split(\"_\")[1])\n",
    "            # Insert predicted tempo at start of each bar if model is provided\n",
    "            if tempo_model:\n",
    "                tempo_feat = tempo_scaler.transform([[bar_num, 0]])\n",
    "                bpm = int(tempo_model.predict(tempo_feat)[0])\n",
    "                s.append(m21tempo.MetronomeMark(number=bpm))\n",
    "        elif tok.startswith(\"Position_\"):\n",
    "            pos = int(tok.split(\"_\")[1])\n",
    "            current_position = bar_num * 4.0 + pos * quarter_length_per_step\n",
    "        elif tok.startswith(\"Track_\"):\n",
    "            track = tok.split(\"_\")[1]\n",
    "        elif tok.startswith(\"Note_\"):\n",
    "            pitch = tok.replace(\"Note_\", \"\")\n",
    "            # Predict expressive duration if model and encoder are provided\n",
    "            if duration_model and encoder:\n",
    "                feat = encoder.transform([[track, pos, pitch]])[0].reshape(1, -1)\n",
    "                qlen = duration_model.predict(feat)[0]\n",
    "                qlen = max(0.25, min(4.0, qlen))\n",
    "            else:\n",
    "                qlen = quarter_length_per_step\n",
    "            # Predict expressive velocity if model and encoder are provided\n",
    "            if velocity_model and velocity_encoder:\n",
    "                feat_v = velocity_encoder.transform([[track, pos, pitch]])[0].reshape(1, -1)\n",
    "                velocity = int(velocity_model.predict(feat_v)[0])\n",
    "            else:\n",
    "                velocity = 64\n",
    "            # Predict articulation (0=staccato, 1=legato)\n",
    "            if articulation_model and articulation_encoder:\n",
    "                feat_a = articulation_encoder.transform([[track, pos, pitch]])[0].reshape(1, -1)\n",
    "                legato = articulation_model.predict(feat_a)[0] > 0.5\n",
    "            else:\n",
    "                legato = False\n",
    "            if track == \"Melody\":\n",
    "                n = m21note.Note(pitch)\n",
    "                # Articulation shaping\n",
    "                if legato:\n",
    "                    n.tie = m21note.Tie(\"start\")\n",
    "                    n.quarterLength = qlen * 1.2\n",
    "                else:\n",
    "                    n.quarterLength = qlen * 0.8\n",
    "                # Optional swing or rubato\n",
    "                swing_offset = (pos % 2) * 0.05  # swing: offset every other 8th\n",
    "                current_position += swing_offset\n",
    "                n.offset = current_position\n",
    "                n.volume.velocity = velocity\n",
    "                parts[\"Melody\"].append(n)\n",
    "            elif track == \"Bass\":\n",
    "                n = m21note.Note(pitch)\n",
    "                # Articulation shaping\n",
    "                if legato:\n",
    "                    n.tie = m21note.Tie(\"start\")\n",
    "                    n.quarterLength = qlen * 1.2\n",
    "                else:\n",
    "                    n.quarterLength = qlen * 0.8\n",
    "                # Optional swing or rubato\n",
    "                rubato_shift = (bar_num % 3) * 0.02  # slow cyclic rubato\n",
    "                current_position += rubato_shift\n",
    "                n.offset = current_position\n",
    "                n.volume.velocity = velocity\n",
    "                parts[\"Bass\"].append(n)\n",
    "        elif tok.startswith(\"Chord_\"):\n",
    "            root = tok.replace(\"Chord_\", \"\")\n",
    "            c = m21chord.Chord([root + \"3\", root + \"4\", root + \"5\"])\n",
    "            c.quarterLength = quarter_length_per_step\n",
    "            c.offset = current_position\n",
    "            parts[\"Chords\"].append(c)\n",
    "    for p in parts.values():\n",
    "        s.append(p)\n",
    "    s.write('midi', fp=filename)\n",
    "    print(f\"Saved MIDI to {filename}\")\n",
    "# Optional render\n",
    "!fluidsynth -ni /usr/share/sounds/sf2/FluidR3_GM.sf2 symbolic_conditioned_structured.mid -F structured_output.wav -r 16000\n",
    "print(\"✅ Saved musically structured Task 1 output to 'symbolic_conditioned_structured.mid' and 'structured_output.wav'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9333f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Assuming 'structured_output.wav' was successfully created by the previous cell\n",
    "# Check if the file exists before attempting to display the audio\n",
    "import os\n",
    "audio_file_path = \"structured_output.wav\"\n",
    "\n",
    "if os.path.exists(audio_file_path):\n",
    "    print(f\"Attempting to play: {audio_file_path}\")\n",
    "    display(Audio(filename=audio_file_path))\n",
    "else:\n",
    "    print(f\"Error: Audio file not found at {audio_file_path}. Please ensure it was created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48a4cb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Step 3: Feature-based generation (variable pitch/duration/interval) ---\n",
    "print(\"🎼 Generating 1-minute sequence with variable pitch/duration/interval (feature-based)...\")\n",
    "import numpy as np\n",
    "import torch\n",
    "from music21 import stream, note as m21note\n",
    "\n",
    "# We'll use the first feature sequence as a starting seed\n",
    "feature_seq_len = 16\n",
    "feature_start_idx = np.random.randint(0, len(input_features))\n",
    "current_input_seq = torch.tensor(input_features[feature_start_idx][:feature_seq_len], dtype=torch.float32).unsqueeze(0)\n",
    "# Dummy model for demonstration (replace with your trained model)\n",
    "class DummyFeatureModel(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Simulate output: random next pitch class\n",
    "        batch, seq, feats = x.shape\n",
    "        return torch.randn(batch, note_encoder.classes_.shape[0])\n",
    "feature_model = DummyFeatureModel()\n",
    "\n",
    "# Generation loop: generate variable-length sequence\n",
    "generated_pitches = []\n",
    "generated_durations = []\n",
    "max_length = 64\n",
    "for step in range(max_length):\n",
    "    out = feature_model(current_input_seq)\n",
    "    next_pitch_idx = torch.argmax(out, dim=-1).item()\n",
    "    generated_pitches.append(next_pitch_idx)\n",
    "    # For duration, use the last duration in input or a default value\n",
    "    if current_input_seq.size(1) > 0:\n",
    "        duration_scaled = current_input_seq[0, -1, 1].item()\n",
    "        duration = scaler_dur.inverse_transform([[duration_scaled]])[0, 0]\n",
    "    else:\n",
    "        duration = 0.5\n",
    "    generated_durations.append(duration)\n",
    "    # Prepare next input (shift window and append next prediction)\n",
    "    # Here, we just roll the input and append a dummy new note for demonstration\n",
    "    next_note = current_input_seq[0, -1, :].clone()\n",
    "    next_note[0] = next_pitch_idx\n",
    "    # Just keep the same duration/interval for demonstration\n",
    "    current_input_seq = torch.cat([current_input_seq[:, 1:, :], next_note.view(1, 1, -1)], dim=1)\n",
    "\n",
    "# Decode feature-based sequence to MIDI (variable durations)\n",
    "s = stream.Stream()\n",
    "for i, p in enumerate(generated_pitches):\n",
    "    n = m21note.Note(int(round(p)))\n",
    "    if i < current_input_seq.size(1):\n",
    "        duration_scaled = current_input_seq[0, i, 1].item()\n",
    "        duration = scaler_dur.inverse_transform([[duration_scaled]])[0, 0]\n",
    "    else:\n",
    "        duration = 0.5  # fallback\n",
    "    n.quarterLength = duration\n",
    "    s.append(n)\n",
    "s.write('midi', fp=\"feature_based_output.mid\")\n",
    "print(\"✅ Saved variable-duration output to 'feature_based_output.mid'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fluidsynth\n",
    "import fluidsynth\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Install fluidsynth if not already installed\n",
    "# You can check for its existence first, but reinstalling is usually fine\n",
    "print(\"Installing fluidsynth...\")\n",
    "!apt-get update -qq && apt-get install fluidsynth -qq -y\n",
    "print(\"fluidsynth installation complete.\")\n",
    "\n",
    "# Verify fluidsynth is now in the PATH\n",
    "print(\"Checking fluidsynth path:\")\n",
    "!which fluidsynth\n",
    "\n",
    "# Convert MIDI to audio using the fluidsynth command-line tool\n",
    "# Ensure the output directory exists if needed\n",
    "output_dir = \"/content\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "midi_file = \"/content/feature_based_output.mid\" # Use the correct MIDI file name\n",
    "\n",
    "output_wav = os.path.join(output_dir, \"output.wav\")\n",
    "soundfont_path = \"/usr/share/sounds/sf2/FluidR3_GM.sf2\"  # default SoundFont\n",
    "\n",
    "# Ensure the soundfont exists (already installed by fluid-soundfont-gm)\n",
    "if not os.path.exists(soundfont_path):\n",
    "    print(f\"Error: Soundfont not found at {soundfont_path}. Please check installation.\")\n",
    "    # Handle the missing soundfont error appropriately, e.g., install soundfont\n",
    "    # !apt-get install fluid-soundfont-gm -y\n",
    "    # You might need to check if the soundfont path is correct for the installed package\n",
    "\n",
    "command = f\"fluidsynth -ni -a alsa -m alsa_seq {soundfont_path} {midi_file} -F {output_wav}\"\n",
    "\n",
    "# Execute the command\n",
    "# The '!' prefix in a Jupyter notebook executes the command in the shell\n",
    "print(f\"Executing command: {command}\") # Add print for debugging\n",
    "# Use !command to execute the shell command. Output or errors from the command\n",
    "# itself will be displayed in the notebook output.\n",
    "!{command}\n",
    "\n",
    "# Check if the output file was created\n",
    "if os.path.exists(output_wav):\n",
    "    print(f\"✅ Successfully created: {output_wav}\")\n",
    "    # Play the result\n",
    "    # Pass the filename explicitly to ipd.Audio\n",
    "    print(f\"Attempting to play: {output_wav}\") # Add print for debugging\n",
    "    display(ipd.Audio(filename=output_wav)) # Use display to ensure it renders in notebook\n",
    "else:\n",
    "    print(f\"❌ Error: Output WAV file was not created at {output_wav}.\")\n",
    "    print(\"Please check the fluidsynth command output above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a1983",
   "metadata": {},
   "source": [
    "## 5. Audio rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb68cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from music21 import stream, note as m21note, chord as m21chord\n",
    "def decode_to_midi(tokens, filename=\"multi_instrument_output.mid\"):\n",
    "    s = stream.Score()\n",
    "    parts = {\"Melody\": stream.Part(), \"Chords\": stream.Part(), \"Bass\": stream.Part()}\n",
    "    # REMI-style bar/position time tracking\n",
    "    bar_num = 0\n",
    "    current_position = 0.0\n",
    "    steps_per_bar = 16\n",
    "    # Patch: double the duration per position for more realistic phrasing\n",
    "    quarter_length_per_step = (4.0 / steps_per_bar) * 2.0\n",
    "    track = None\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"Bar_\"):\n",
    "            bar_num = int(tok.split(\"_\")[1])\n",
    "        elif tok.startswith(\"Position_\"):\n",
    "            pos = int(tok.split(\"_\")[1])\n",
    "            current_position = bar_num * 4.0 + pos * quarter_length_per_step\n",
    "        elif tok.startswith(\"Track_\"):\n",
    "            track = tok.split(\"_\")[1]\n",
    "        elif tok.startswith(\"Note_\"):\n",
    "            pitch = tok.replace(\"Note_\", \"\")\n",
    "            if track == \"Melody\":\n",
    "                n = m21note.Note(pitch)\n",
    "                n.quarterLength = quarter_length_per_step\n",
    "                n.offset = current_position\n",
    "                parts[\"Melody\"].append(n)\n",
    "            elif track == \"Bass\":\n",
    "                n = m21note.Note(pitch)\n",
    "                n.quarterLength = quarter_length_per_step\n",
    "                n.offset = current_position\n",
    "                parts[\"Bass\"].append(n)\n",
    "        elif tok.startswith(\"Chord_\"):\n",
    "            root = tok.replace(\"Chord_\", \"\")\n",
    "            c = m21chord.Chord([root + \"3\", root + \"4\", root + \"5\"])\n",
    "            c.quarterLength = quarter_length_per_step\n",
    "            c.offset = current_position\n",
    "            parts[\"Chords\"].append(c)\n",
    "    for p in parts.values():\n",
    "        s.append(p)\n",
    "    s.write('midi', fp=filename)\n",
    "    print(f\"Saved MIDI to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new long version\n",
    "decode_to_midi(\n",
    "    long_tokens,\n",
    "    filename=\"symbolic_conditioned.mid\",\n",
    "    duration_model=duration_model,\n",
    "    encoder=encoder,\n",
    "    velocity_model=velocity_model,\n",
    "    velocity_encoder=velocity_encoder,\n",
    "    articulation_model=articulation_model,\n",
    "    articulation_encoder=articulation_encoder,\n",
    "    tempo_model=tempo_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c412ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fluidsynth\n",
    "import fluidsynth\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Install fluidsynth if not already installed\n",
    "# You can check for its existence first, but reinstalling is usually fine\n",
    "print(\"Installing fluidsynth...\")\n",
    "!apt-get update -qq && apt-get install fluidsynth -qq -y\n",
    "print(\"fluidsynth installation complete.\")\n",
    "\n",
    "# Verify fluidsynth is now in the PATH\n",
    "print(\"Checking fluidsynth path:\")\n",
    "!which fluidsynth\n",
    "\n",
    "# Convert MIDI to audio using the fluidsynth command-line tool\n",
    "# Ensure the output directory exists if needed\n",
    "output_dir = \"/content\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "midi_file = \"/content/symbolic_conditioned.mid\" # Use the correct MIDI file name\n",
    "\n",
    "output_wav = os.path.join(output_dir, \"output.wav\")\n",
    "soundfont_path = \"/usr/share/sounds/sf2/FluidR3_GM.sf2\"  # default SoundFont\n",
    "\n",
    "# Ensure the soundfont exists (already installed by fluid-soundfont-gm)\n",
    "if not os.path.exists(soundfont_path):\n",
    "    print(f\"Error: Soundfont not found at {soundfont_path}. Please check installation.\")\n",
    "    # Handle the missing soundfont error appropriately, e.g., install soundfont\n",
    "    # !apt-get install fluid-soundfont-gm -y\n",
    "    # You might need to check if the soundfont path is correct for the installed package\n",
    "\n",
    "command = f\"fluidsynth -ni -a alsa -m alsa_seq {soundfont_path} {midi_file} -F {output_wav}\"\n",
    "\n",
    "# Execute the command\n",
    "# The '!' prefix in a Jupyter notebook executes the command in the shell\n",
    "print(f\"Executing command: {command}\") # Add print for debugging\n",
    "# Use !command to execute the shell command. Output or errors from the command\n",
    "# itself will be displayed in the notebook output.\n",
    "!{command}\n",
    "\n",
    "# Check if the output file was created\n",
    "if os.path.exists(output_wav):\n",
    "    print(f\"✅ Successfully created: {output_wav}\")\n",
    "    # Play the result\n",
    "    # Pass the filename explicitly to ipd.Audio\n",
    "    print(f\"Attempting to play: {output_wav}\") # Add print for debugging\n",
    "    display(ipd.Audio(filename=output_wav)) # Use display to ensure it renders in notebook\n",
    "else:\n",
    "    print(f\"❌ Error: Output WAV file was not created at {output_wav}.\")\n",
    "    print(\"Please check the fluidsynth command output above for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎼 Generating extended symbolic sequence for Task 1 (1-minute duration)...\")\n",
    "\n",
    "# Generate a long sequence (e.g., 1200 tokens)\n",
    "start_tokens = random.choice(encoded_token_seqs)[:256]\n",
    "long_tokens_task1_long = generate_sequence(start_tokens, transformer_model, vocab, inv_vocab, length=1800)\n",
    "\n",
    "# Save to MIDI\n",
    "decode_to_midi(\n",
    "    long_tokens_task1_long,\n",
    "    filename=\"extended_symbolic_task1_long.mid\",\n",
    "    duration_model=duration_model,\n",
    "    encoder=encoder,\n",
    "    velocity_model=velocity_model,\n",
    "    velocity_encoder=velocity_encoder,\n",
    "    articulation_model=articulation_model,\n",
    "    articulation_encoder=articulation_encoder,\n",
    "    tempo_model=tempo_model\n",
    ")\n",
    "\n",
    "# Convert to WAV\n",
    "!fluidsynth -ni /usr/share/sounds/sf2/FluidR3_GM.sf2 extended_symbolic_task1_long.mid -F extended_task1_long.wav -r 16000\n",
    "print(\"✅ Saved extended Task 1 symbolic music to 'extended_symbolic_task1_long.mid' and audio to 'extended_task1_long.wav'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34039b8",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# Convert MIDI to audio using the fluidsynth command-line tool\n",
    "# Ensure the output directory exists if needed\n",
    "output_dir = \"/content\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "midi_file = \"/content/extended_symbolic_task1_long.mid\" # Use the correct MIDI file name\n",
    "\n",
    "output_wav = os.path.join(output_dir, \"output.wav\")\n",
    "soundfont_path = \"/usr/share/sounds/sf2/FluidR3_GM.sf2\"  # default SoundFont\n",
    "\n",
    "# Ensure the soundfont exists (already installed by fluid-soundfont-gm)\n",
    "if not os.path.exists(soundfont_path):\n",
    "    print(f\"Error: Soundfont not found at {soundfont_path}. Please check installation.\")\n",
    "    # Handle the missing soundfont error appropriately, e.g., install soundfont\n",
    "    # !apt-get install fluid-soundfont-gm -y\n",
    "    # You might need to check if the soundfont path is correct for the installed package\n",
    "\n",
    "command = f\"fluidsynth -ni -a alsa -m alsa_seq {soundfont_path} {midi_file} -F {output_wav}\"\n",
    "\n",
    "# Execute the command\n",
    "# The '!' prefix in a Jupyter notebook executes the command in the shell\n",
    "print(f\"Executing command: {command}\") # Add print for debugging\n",
    "# Use !command to execute the shell command. Output or errors from the command\n",
    "# itself will be displayed in the notebook output.\n",
    "!{command}\n",
    "\n",
    "# Check if the output file was created\n",
    "if os.path.exists(output_wav):\n",
    "    print(f\"✅ Successfully created: {output_wav}\")\n",
    "    # Play the result\n",
    "    # Pass the filename explicitly to ipd.Audio\n",
    "    print(f\"Attempting to play: {output_wav}\") # Add print for debugging\n",
    "    display(ipd.Audio(filename=output_wav)) # Use display to ensure it renders in notebook\n",
    "else:\n",
    "    print(f\"❌ Error: Output WAV file was not created at {output_wav}.\")\n",
    "    print(\"Please check the fluidsynth command output above for errors.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
