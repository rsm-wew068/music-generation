{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36803b80",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41bb30",
   "metadata": {},
   "source": [
    "## Part 1: Playlist Continuation\n",
    "\n",
    "In the first part of this homework, you'll build a simple music playlist continuation system based on collaborative filtering and audio features using a dataset derived from the [Million Playlist Dataset (MPD)](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge#challenge-dataset). \n",
    "\n",
    "Later in this part, you will test each of your systems based on these four playlists:\n",
    "\n",
    "- Playlist 1:\n",
    "    - Britney Spears \"...Baby One More Time\"\n",
    "    - Kelly Clarkson \"Since U Been Gone\"\n",
    "- Playlist 2:\n",
    "    - Radiohead \"Exit Music\",\n",
    "    - Muse \"Citizen Erased\"\n",
    "- Playlist 3:\n",
    "    - Dr. Dre \"Xxplosive\"\n",
    "    - Eric B. & Rakim \"Paid In Full\"\n",
    "- Playlist 4:\n",
    "    - Rage Against The Machine \"Bombtrack\"\n",
    "    - Audioslave \"Like A Stone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46954395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12235aec",
   "metadata": {},
   "source": [
    "### Unzip embeddings.zip\n",
    "This compressed file contains a list of audio embeddings, extracted from MusiCNN (Pons and Serra, 2019)\n",
    "\n",
    "Each file is named as `[tid].npy` format.\n",
    "\n",
    "For example, `TRCSBUG128F422318C.npy` is an audio embedding of \"The Navy Song\" by Billy Talent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip embeddings.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05e4ef",
   "metadata": {},
   "source": [
    "## 1. Construct samples using the training set (1 mark)\n",
    "\n",
    "**Inputs**\n",
    "- playlists: a dictionary where each key is a playlist index and the corresponding value is a list of track dictionaries. Each track dictionary contains metadata such as `tid`, `artist_name`, and `track_name`.\n",
    "\n",
    "**Outputs**\n",
    "- data: a list of interaction tuples. Each tuple contains three elements:\n",
    "    - the playlist index (i.e., user index) (e.g., 0, 1, 2...),\n",
    "    - the track index (mapped from tid) (e.g., 0, 1, 2...),\n",
    "    - a binary label indicating interaction (1 for positive, 0 for negative). For example, the tuple `(0, 0, 1)` means that playlist 0 included track 0, while `(1, 2, 0)` means playlist 1 did not include track 2. The number of positive samples (label = `1`) must be equal to the number of negative samples (label = 0). Since there are more negative samples than positive ones, you should randomly select negative samples.\n",
    "- tid_to_idx: a dictionary mapping each unique tid to a unique index (integer). (e.g., `{'TRPYFRU128F427777C':0 ...}`)\n",
    "- idx_to_tid: the inverse of `tid_to_idx`. A dictionary mapping each unique index back to its corresponding tid. (e.g., `{0: 'TRPYFRU128F427777C' ...}`)\n",
    "- tid_to_meta: a dictionary mapping each tid to its metadata as a tuple: `(artist_name, track_name)`.\n",
    "    For example: `tid_to_meta['TRXIOLV12903CF1882'] = ('The Antlers', 'Wake')`.\n",
    "\n",
    "**One thing to note**\n",
    "- When mapping TIDs to indexes, the order of TIDs should not matter. For example, `{‘TRPYFRU128F427777C': 0, …}` and `{‘TRXIOLV12903CF1882': 0, …}` are both valid `tid_to_idx` mappings. However, the dictionaries `tid_to_idx`,` idx_to_tid`, and `tid_to_meta` must include all TIDs present in the playlists, and the indices should range from `0` to `len(playlists) - 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interaction_samples(playlists):\n",
    "    # Your code here\n",
    "    return data, tid_to_idx, idx_to_tid, tid_to_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70428554",
   "metadata": {},
   "source": [
    "## 2. Implement a PyTorch dataset and dataloader and train the provided WRMF model (this part is given)\n",
    "\n",
    "Implement `WRMFDataset`, a PyTorch dataset that takes a data index as input and returns the corresponding playlist index, track index, and binary interaction label.\n",
    "Then, optimize the provided WRMF model for 10 epochs. Use the model’s `compute_loss` method when calculating the training loss (1 mark).\n",
    "\n",
    "**Inputs**\n",
    "- `data`: A list of interaction tuples created with `build_interaction_samples`\n",
    "\n",
    "**Outputs**\n",
    "- `playlist_index`: An index corresponding to the playlist\n",
    "- `track_index`: An index corresponding to the TID\n",
    "- `label`: A binary label (0 or 1) indicating whether the playlist includes the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMFDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return item[0], item[1], item[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73aa45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_factors, alpha=40.0, lambda_reg=0.1):\n",
    "        super(WRMF, self).__init__()\n",
    "        self.user_factors = nn.Embedding(num_users, num_factors)\n",
    "        self.item_factors = nn.Embedding(num_items, num_factors)\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        nn.init.normal_(self.user_factors.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_factors.weight, std=0.01)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
    "\n",
    "    def compute_loss(self, user, item, feedback):\n",
    "        prediction = torch.sigmoid(self.forward(user, item))\n",
    "        confidence = 1 + self.alpha * feedback\n",
    "        loss = (confidence * (1 - feedback - prediction)**2).sum()\n",
    "        loss += self.lambda_reg * (\n",
    "            self.user_factors(user).pow(2).sum() +\n",
    "            self.item_factors(item).pow(2).sum()\n",
    "        )\n",
    "        return loss / user.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705add3",
   "metadata": {},
   "source": [
    "## 3. Recommend using the trained WRMF model (1 mark)\n",
    "\n",
    "#### `get_playlist_embedding()`\n",
    "\n",
    "**Inputs**\n",
    "- `model`: A trained WRMF model\n",
    "- `playlist`: A list of tids representing a playlist\n",
    "- `tid_to_idx`: As implemented above\n",
    "\n",
    "**Output**\n",
    "- `playlist_embedding`: An average tensor of embeddings, where each embedding corresponds to a track in the playlist (Use `item_factors` of the model.)\n",
    "\n",
    "#### `generate_recommendations()`\n",
    "\n",
    "**Inputs**\n",
    "- `model`: A trained WRMF model\n",
    "- `playlist`: A list of tids representing a playlist\n",
    "- `all_item_embeddings`: A tensor containing all track embeddings, retrieved from the trained WRMF model (Use the weights of `item_factors` of the model.)\n",
    "- `idx_to_tid`: As implemented above\n",
    "- `tid_to_idx`: As implemented above\n",
    "- `N`: Number of recommendations to return\n",
    "\n",
    "**Details**\n",
    "- Convert the playlist's tids into embedding indices using `tid_to_idx`\n",
    "- Compute the average playlist embedding using `get_playlist_embedding`\n",
    "- Calculate cosine similarity between the playlist embedding and `all_item_embeddings`\n",
    "- Identify the most similar items\n",
    "\n",
    "**Outputs**\n",
    "- `similarities`: Similarity scores for the 10 most similar items that are not already in the playlist\n",
    "- `recommendations`: A list of track IDs (e.g. \"TRCNVLA12903CF6052\", not indices) for those items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playlist_embedding(model, playlist, tid_to_idx):\n",
    "    # Your code here\n",
    "    return item_embeddings.mean(dim=0)\n",
    "\n",
    "def generate_recommendations(model, playlist, all_item_embeddings, idx_to_tid, tid_to_idx, N):\n",
    "    # Your code here\n",
    "    # (line below is a hint, feel free to delete)\n",
    "    return [similarities[int(idx)].item() for idx in recommended_indices if idx not in playlist_item_ids][:N],\\\n",
    "           [idx_to_tid[int(idx)] for idx in recommended_indices if idx not in playlist_item_ids][:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ead08",
   "metadata": {},
   "source": [
    "# 4. Generate rankings on the test set using the trained WRMF model (2 marks)\n",
    "\n",
    "#### `make_cf_rankings()`\n",
    "\n",
    "**Inputs**\n",
    "- `test_playlists`: A dictionary of playlists loaded from `test_playlists.json`\n",
    "- `all_item_embeddings`: A tensor containing all track embeddings, retrieved from the trained WRMF model\n",
    "\n",
    "**Outputs**\n",
    "- `rankings`: A dictionary where each key is a playlist index, and the corresponding value is a list of track IDs sorted in descending order of similarity to the embedding of the first two tracks in the playlist\n",
    "- `targets`: A dictionary where each key is a playlist index, and the corresponding value is a list of track IDs indices from the third track onward (i.e., tracks that should be recommended)\n",
    "\n",
    "**Details**\n",
    "- Note that the recommendations should be generated based on **the first two tracks** in the playlist and the targets are the third track onward.\n",
    "- You could probably return either ids (0,1,2) or track IDs (\"TRCNVLA12903CF6052\" etc.), as long as you are consistent between both outputs.\n",
    "\n",
    "**Instructions**\n",
    "- Initialize `rankings` and `targets` as empty dictionaries.\n",
    "- For each playlist:\n",
    "    - Filter out any TIDs that do not exist in the training set.\n",
    "    - Obtain the average embedding of the first two tracks in the playlist using `get_playlist_embedding`.\n",
    "    - Calculate the cosine similarity between the playlist embedding and `all_item_embeddings`.\n",
    "    - Sort the results in descending order and exclude tracks already present in the first two tracks of the playlist.\n",
    "    - Store the ranking in the `rankings` dictionary, where the key is the playlist index and the value is the ranking.\n",
    "    - Store the tracks from the third one onward in the `targets` dictionary, where the key is the playlist index and the value is the list of tracks\n",
    "\n",
    "**Outputs**\n",
    "- Return the constructed `rankings` and `targets` dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5785dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cf_rankings(model, test_playlists, all_item_embeddings, idx_to_tid, tid_to_idx):\n",
    "    # Your code here\n",
    "    return rankings, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c37e56",
   "metadata": {},
   "source": [
    "## 5. Evaluate using MRR (1 mark)\n",
    "\n",
    "### `get_mrr()`\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "- `rankings`: A dictionary mapping each playlist index to a list of recommended item indices, sorted by predicted relevance.\n",
    "- `targets`: A dictionary mapping each playlist index to a list of ground-truth item indices (i.e., tracks that should have been recommended).\n",
    "\n",
    "**Output**\n",
    "\n",
    "- `mrr`: A float value representing the **Mean Reciprocal Rank (MRR)** across all playlists.\n",
    "\n",
    "**Details**\n",
    "\n",
    "- MRR measures how early in the recommendation list the correct items appear. For each target item that appears in the ranking, we compute the reciprocal of its rank (1-based). The MRR for a single playlist is the average reciprocal rank of all its targets. The final output is the mean over all playlists.\n",
    "\n",
    "**Formula:**\n",
    "Let $R_i$ be the rank of the $i$-th correct (target) item in the recommended list. For a playlist with $n$ correct items:\n",
    "\n",
    "$$\\text{MRR}_{\\text{playlist}} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{R_i}$$\n",
    "\n",
    "$$\\text{MRR} = \\frac{1}{|\\text{Playlists}|} \\sum_{\\text{playlist}} \\text{MRR}_{\\text{playlist}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrr(rankings, targets):\n",
    "    # Your code here\n",
    "    return np.average(mrrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522c547",
   "metadata": {},
   "source": [
    "## 6. Evaluate using Precision (1 mark)\n",
    "\n",
    "### `get_precision()`\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "- `rankings`: A dictionary mapping each playlist index to a list of recommended item indices.\n",
    "- `targets`: A dictionary mapping each playlist index to a list of ground-truth item indices.\n",
    "\n",
    "**Output**\n",
    "\n",
    "- `precision`: A float value representing **Precision\\@10**, averaged over all playlists.\n",
    "\n",
    "**Details**\n",
    "* Precision\\@10 evaluates how many of the top 10 recommended items are relevant. For each playlist, it computes the number of recommended items in the top 10 that are also in the ground-truth list, divided by the number of ground-truth items.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\text{Precision@10} = \\frac{|\\text{Top-10 Recommendations} \\cap \\text{Targets}|}{|\\text{Targets}|}$$\n",
    "\n",
    "$$\\text{Precision} = \\frac{1}{|\\text{Playlists}|} \\sum_{\\text{playlist}} \\text{Precision@10}_{\\text{playlist}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314dddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(rankings, targets):\n",
    "    # Your code here\n",
    "    return np.average(precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a344d25",
   "metadata": {},
   "source": [
    "## 7. Recommend Based on Audio Similarity (1 mark)\n",
    "\n",
    "### `get_average_audio_embeddings()`\n",
    "\n",
    "**Inputs**\n",
    "- `playlist`: A list of track IDs (`tid`s) representing a playlist\n",
    "- `embedding_directory`: A directory path containing `.npy` files, each named after a `tid` and containing audio embeddings.\n",
    "\n",
    "**Output**\n",
    "- A single averaged audio embedding vector (NumPy array) representing the playlist\n",
    "\n",
    "**Details**\n",
    "- For each track in the playlist, load its audio embedding from the corresponding `.npy` file and compute the mean along the time axis. Then return the average of all such track embeddings as a single playlist-level embedding.\n",
    "\n",
    "\n",
    "### `get_embeddings()`\n",
    "\n",
    "**Inputs**\n",
    "- `embedding_directory`: Path to a directory containing `.npy` files. Each file represents the audio embedding of a track and is named with its corresponding `tid`.\n",
    "\n",
    "**Outputs**\n",
    "- `embedding_matrix`: A NumPy array where each row is the mean audio embedding of a track\n",
    "- `tids`: A list of `tid`s corresponding to each row in `embedding_matrix`\n",
    "\n",
    "**Details**\n",
    "- Loads all `.npy` files from the directory\n",
    "- For each file, computes the average embedding across time (i.e., mean over axis 0)\n",
    "- Returns the stacked embeddings as a matrix and the list of corresponding `tid`s\n",
    "\n",
    "\n",
    "### `get_similarity()`\n",
    "\n",
    "**Inputs**\n",
    "- `playlist`: A list of `tid`s representing the input playlist\n",
    "- `playlist_embedding`: A single embedding vector (e.g., from `get_average_audio_embeddings`)\n",
    "- `embedding_matrix`: A NumPy array of all track embeddings (output from `get_embeddings`)\n",
    "- `tids`: A list of `tid`s corresponding to the rows of `embedding_matrix`\n",
    "\n",
    "**Output**\n",
    "- `rankings`: A list of track indices (mapped from `tid` using `tid_to_idx`), sorted by descending cosine similarity to the input playlist embedding, and excluding tracks already in the input playlist\n",
    "\n",
    "**Details**\n",
    "- Computes cosine similarity between the `playlist_embedding` and all track embeddings in `embedding_matrix`\n",
    "- Sorts the results in descending order of similarity\n",
    "- Filters out tracks that are already in the playlist\n",
    "- Return similarities and rankings (similar to `generate_recommendations` above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_audio_embedding(playlist, embedding_directory):\n",
    "    # Your code here\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab286b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(embedding_directory, tids):\n",
    "    # Your code here\n",
    "    return embedding_matrix, tids\n",
    "\n",
    "def get_similarity(playlist, playlist_embedding, embedding_matrix, tid_to_idx, tids):\n",
    "    # Your code here\n",
    "    return sims, rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a279777",
   "metadata": {},
   "source": [
    "## 8. Generate rankings on the test set using audio similarity (1 mark)\n",
    "\n",
    "### `make_audio_inference()`\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "- `test_playlists`: A dictionary where each key is a playlist index and the corresponding value is a list of track dictionaries. Each track dictionary must contain a `'tid'` field.\n",
    "- `embedding_matrix`: A NumPy array of all track embeddings (output from `get_embeddings`)\n",
    "- `tids`: A list of `tid`s corresponding to the rows of `embedding_matrix`\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "- `rankings`: A dictionary where each key is a playlist index and the corresponding value is a list of recommended tracks (not present in the input playlist), sorted in descending order of cosine similarity to the audio-based playlist embedding\n",
    "- `targets`: A dictionary where each key is a playlist index and the corresponding value is a list of ground-truth tracks (i.e., tracks from the third position onward in the playlist)\n",
    "\n",
    "**Details**\n",
    "\n",
    "- For each playlist:\n",
    "  - Use the first two tracks to compute the playlist-level audio embedding using `get_average_audio_embeddings`\n",
    "  - Generate a ranked list of similar tracks (excluding existing ones) using `get_similarity`\n",
    "  - Store the ranking in `rankings` and the remaining tracks (from position 3 onward) in `targets`\n",
    "  - As above, could return either tracks or IDs, as long as you're consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_audio_rankings(directory, test_playlists, embedding_matrix, tid_to_idx, tids):\n",
    "    # Your code here\n",
    "    return rankings, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03efbd6",
   "metadata": {},
   "source": [
    "## Part 2: Sound Synthesis\n",
    "\n",
    "In the second part of this homework, you'll implement some basic utilities of a sound synthesizer (ADSR and LFOs). Specifically, you'll modify the amplitude of a sound using ADSR and the pitch using an LFO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b8947",
   "metadata": {},
   "source": [
    "## 9. Implement ADSR envelope (1 mark).\n",
    "\n",
    "- Attack Time: How long it takes for the synth to reach maximum amplitude (i.e., 1).\n",
    "- Decay Time: How long it takes for the synth to decrease to the sustain level after the attack phase.\n",
    "- Sustain Level: The amplitude level maintained after the decay phase while the note is held.\n",
    "- Release Time: How long it takes for the synth to fade out completely after the note is released.\n",
    "\n",
    "#### `adsr_envelope()`\n",
    "\n",
    "**Inputs**\n",
    "* duration: The duration (in seconds) during which the note is held.\n",
    "* attack_time: A float representing the attack time (in seconds).\n",
    "* decay_time: A float representing the decay time (in seconds).\n",
    "* sustain_level: A float representing the sustain level, ranging from 0 to 1.\n",
    "* release_time: A float representing the release time (in seconds).\n",
    "\n",
    "**Outputs**\n",
    "* envelope: A 1D NumPy array of floating-point values between 0 and 1, with a total length of int((duration + release_time) * sr) samples. This array is intended to be multiplied with a waveform (with constant amplitude 1) to apply the ADSR envelope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3985a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adsr_envelope(duration, attack_time, decay_time, sustain_level, release_time, sr=44100):\n",
    "    # Your code here\n",
    "    return envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56143aaf",
   "metadata": {},
   "source": [
    "### 10. Modulate the cutoff frequency of the low-pass filter using an LFO (1 mark)\n",
    "\n",
    "- Base Cutoff Frequency: The central or starting cutoff frequency.\n",
    "- LFO Frequency: How fast the LFO oscillates. For example, A 0.5 Hz LFO periodically modulates the cutoff once every 2 seconds.\n",
    "- LFO Depth: The amount of intensity by which the LFO affects the cutoff frequency. For example, if the cutoff is 200Hz, the actual cutoff will swing between 800Hz and 1200Hz.\n",
    "\n",
    "![](./question10.png)\n",
    "\n",
    "#### `get_lfo()`\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "* `base_cutoff`: A float representing the base cutoff frequency in Hz.\n",
    "* `lfo_depth`: A float representing the depth of modulation in Hz.\n",
    "* `lfo_freq`: A float representing the frequency of the LFO in Hz.\n",
    "* `t`: A 1D NumPy array representing evenly spaced time values over the duration of the signal. The number of samples is determined by the sampling rate and the total duration.\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "* `lfo`: A 1D NumPy array of the same length as `t`, where each element represents the modulated cutoff frequency at that point in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6430659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lfo(base_cutoff, lfo_depth, lfo_freq, t):\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1d1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
